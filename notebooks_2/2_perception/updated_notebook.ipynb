{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778ee99-3e37-4c80-8927-c302f2644d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get jp a prompt to animation api -> 5 animations - 4 hours - end at 6:30\n",
    "# new hashirama.com = animation api  - static caddy hashirama.html\n",
    "#  parse papers -> get summaries  -> sentence -> gif\n",
    "# youtube / audio -> summarizier -> make giffs out of sentences\n",
    "# type blog = asldfkasdlfkasf -> every kepress = new gif -> edit the pile of gifs and share them\n",
    "# use flux + all tricks \n",
    "\n",
    "\n",
    "#! wget https://silverfallsseed.com/wp-content/uploads/2015/12/cOSMOS-dAYDREAM-FIELD-20201-PETES-14.jpg\n",
    "\n",
    "#! mv  cOSMOS-dAYDREAM-FIELD-20201-PETES-14.jpg flower\n",
    "\n",
    "#! mv flower flower.jpg\n",
    "\n",
    "\n",
    "#import pixels2svg\n",
    "#dir(   pixels2svg.__version__)\n",
    "# https://github.com/ValentinFrancois/pixels2svg/blob/main/examples/sword_outline.py\n",
    "# https://potrace.sourceforge.net/\n",
    "# https://github.com/ValentinFrancois/pixels2svg/blob/main/examples/sword_outline.py\n",
    "# https://www.google.com/search?q=sentence+transformers&oq=sentence+transformers&sourceid=chrome&ie=UTF-8\n",
    "# https://catherineh.github.io/programming/2018/04/04/pngs-to-embroidery-patterns\n",
    "# https://threejs.org/examples/?q=webgpu#webgpu_postprocessing_transition\n",
    "\n",
    "#https://replicate.com/explore?latest_models_page=2#latest-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cfa1c1-dd88-4612-ac84-aed5ec6a7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "##https://github.com/barbagroup/AeroPython\n",
    "\n",
    "# jeremey howard\n",
    "# peter norvig\n",
    "# lucumr\n",
    "# simonw\n",
    "# https://github.com/thombashi/sqlitebiter\n",
    "# https://github.com/topics/python\n",
    "# https://pypi.org/project/pixels2svg/\n",
    "# https://medium.com/@anand.butani/from-pixels-to-vectors-mastering-image-to-svg-conversion-with-python-eca75805b247\n",
    "# https://pypi.org/project/svglib/\n",
    "# https://github.com/mapbox/earcut\n",
    "# https://medium.com/skymod-tech/re-birth-of-the-game-changer-sam-2-segment-anything-model-2-by-meta-ai-6b9a5a9f50b9\n",
    "# https://github.com/facebookresearch/segment-anything-2/blob/main/notebooks/video_predictor_example.ipynb\n",
    "# https://github.com/facebookresearch/segment-anything-2/blob/main/notebooks/image_predictor_example.ipynb\n",
    "# https://github.com/facebookresearch/segment-anything-2/blob/main/notebooks/automatic_mask_generator_example.ipynb\n",
    "# https://docs.ultralytics.com/usage/python/#track\n",
    "# https://docs.ultralytics.com/hub/quickstart/\n",
    "# https://docs.ultralytics.com/guides/object-counting/\n",
    "# https://docs.ultralytics.com/guides/heatmaps/#real-world-applications\n",
    "# https://tailwindui.com/components/application-ui/data-display/calendars\n",
    "# https://observablehq.com/@mbostock/packing-circles-inside-a-rectangle\n",
    "# https://github.github.com/gfm/\n",
    "# https://x.com/flornkm/status/1835655689936445948\n",
    "# https://threejs.org/examples/css3d_periodictable\n",
    "# https://replicate.com/yorickvp/llava-v1.6-mistral-7b\n",
    "# https://replicate.com/yorickvp/llava-13b\n",
    "# https://replicate.com/zsxkib/whisper-lazyloading\n",
    "# https://medium.com/skymod-tech/re-birth-of-the-game-changer-sam-2-segment-anything-model-2-by-meta-ai-6b9a5a9f50b9\n",
    "# https://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file\n",
    "# https://github.com/facebookresearch/segment-anything-2/blob/main/notebooks/image_predictor_example.ipynb\n",
    "# https://github.com/facebookresearch/segment-anything-2/blob/main/notebooks/image_predictor_example.ipynb\n",
    "# https://huggingface.co/models\n",
    "# https://replicate.com/ayumuakagi/segment_anything_model?prediction=jbt36s09ndrg80chzjhrtcgfp8\n",
    "# https://github.com/facebookresearch/segment-anything-2/blob/main/notebooks/video_predictor_example.ipynb\n",
    "# http://localhost:8000/every-research-paper-zoox-waymo-visualized\n",
    "# https://replicate.com/daanelson/yolox\n",
    "# https://replicate.com/adirik/grounding-dino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47a1c315-4da4-4687-9c8f-0218e2ccef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "current_notebook = \"./lottie_gif.ipynb\"\n",
    "current_ =json.load(open(current_notebook))\n",
    "\n",
    "current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe2611df-2ced-4165-aae0-48d866004a3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnbformat\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Get the current notebook object\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m nb \u001b[38;5;241m=\u001b[39m \u001b[43mnbformat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Iterate over the cells and delete empty ones\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(nb\u001b[38;5;241m.\u001b[39mcells):\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/nbformat/v4/nbjson.py:30\u001b[0m, in \u001b[0;36mJSONReader.reads\u001b[0;34m(self, s, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreads\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read a JSON string into a Notebook object\"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     nb \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     nb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_notebook(nb, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nb\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/json/__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[0;32m--> 339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    340\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not dict"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "# Get the current notebook object\n",
    "nb = nbformat.v4.reads(current_, as_version=4)\n",
    "\n",
    "# Iterate over the cells and delete empty ones\n",
    "for i, cell in enumerate(nb.cells):\n",
    "    if not cell.source:  # Check if cell is empty (no source)\n",
    "        del nb.cells[i]  # Delete the cell\n",
    "\n",
    "# Write the updated notebook to a new file or overwrite the original one\n",
    "nbformat.write(nb, 'updated_notebook.ipynb')  # Change the filena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b096084-71c8-4689-b6b5-b77c15ad91fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629f641821534f59863ed642be18ebce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1a0cc192dc4819bc96fb7e3b4352eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b84f317f0944c59469254504a2239d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max mem allocated (GB) while denoising: 2.390900135040283\n"
     ]
    }
   ],
   "source": [
    "from diffusers import FluxPipeline\n",
    "import torch\n",
    "\n",
    "ckpt_id = \"black-forest-labs/FLUX.1-schnell\"\n",
    "prompt = [\n",
    "    \"an astronaut riding a rhino\",\n",
    "    # more prompts here\n",
    "]\n",
    "height, width = 1024, 1024\n",
    "def working_flux():\n",
    "# denoising\n",
    "    pipe = FluxPipeline.from_pretrained(\n",
    "        ckpt_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    pipe.vae.enable_tiling()\n",
    "    pipe.vae.enable_slicing()\n",
    "    pipe.enable_sequential_cpu_offload() # offloads modules to CPU on a submodule level (rather than model level)\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=1,\n",
    "        guidance_scale=0.0,\n",
    "        height=height,\n",
    "        width=width,\n",
    "    ).images[0]\n",
    "#print('Max mem allocated (GB) while denoising:', torch.cuda.max_memory_allocated() / (1024 ** 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a23101b7-4464-4d34-b3fb-be2b50f2124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image\n",
    "#https://replicate.com/collections/utilities\n",
    "#https://replicate.com/collections/image-editing\n",
    "#https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux\n",
    "#https://huggingface.co/black-forest-labs/FLUX.1-schnell/discussions/5#:~:text=Depends%20how%20fast%20you%20want,%2D%202%20bytes%20%2F%20param))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe9188-1e38-413e-87c7-583f8580b017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f936f143-dd0c-4702-b02c-44f4e9aa82be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d81f481-6396-4407-9e84-50b219fe83bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01e91154-ebcd-4a23-9fd9-08d82f788536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install transformers timm torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda56fab-4dfa-4a61-a455-83295e1a4cbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'raster_to_svg' from 'pixels2svg' (/home/adnan/micromamba/envs/sam/lib/python3.10/site-packages/pixels2svg-0.2.2-py3.10.egg/pixels2svg/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpixels2svg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m raster_to_svg\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'raster_to_svg' from 'pixels2svg' (/home/adnan/micromamba/envs/sam/lib/python3.10/site-packages/pixels2svg-0.2.2-py3.10.egg/pixels2svg/__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b876f8a8-88c7-4eed-b5b6-bcb9e8a6ce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img count  7068\n",
      "38441\n"
     ]
    }
   ],
   "source": [
    "from pixels2svg import pixels2svg\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "#https://github.com/mapbox/earcut\n",
    "#https://news.ycombinator.com/item?id=41600177\n",
    "# https://github.com/tdh8316/triangler\n",
    "\n",
    "\n",
    "\n",
    "def _image_to_svg(image_path, i=0):\n",
    "    # Load an image using OpenCV\n",
    "    print(i)\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load the image in grayscale\n",
    "    # Display the loaded image (optional)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    # Save the SVG output to a file\n",
    "    output_file = \"output_2.svg\"\n",
    "    #from pixels2svg import pixels2svg\n",
    "\n",
    "    # with open(output_file, \"w\") as file:\n",
    "    #     file.write(svg_output)\n",
    "    # print(f\"SVG file saved as {output_file}\")\n",
    "    # Convert the image to SVG\n",
    "    svg_output =  pixels2svg(image_path, f\"output_{i}.svg\")\n",
    "\n",
    "    #svg_output = raster_to_svg(image, num_paths=100, path_size=1.0, error_threshold=0.1)\n",
    "    # Print the SVG output to the console (optional)\n",
    "    print(svg_output)\n",
    "\n",
    "\n",
    "import base64\n",
    "import mimetypes\n",
    "import os.path\n",
    "\n",
    "from svgwrite.image import Image\n",
    "from svgwrite.masking import ClipPath\n",
    "\n",
    "#from examples.base import BRAIN_OVERLAY_PNG_PATH, BRAIN_PNG_PATH\n",
    "\n",
    "from pixels2svg import Drawing, pixels2svg\n",
    "\n",
    "def file_to_base64(filepath):\n",
    "    \"\"\"Returns the content of a file as a Base64 encoded string.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        encoded_str = base64.b64encode(f.read())\n",
    "    return encoded_str.decode('utf-8')\n",
    "\n",
    "\n",
    "def file_to_base64_html(filepath):\n",
    "    mime_type = mimetypes.guess_type(filepath)[0]\n",
    "    base64_data = file_to_base64(filepath)\n",
    "    return f'data:{mime_type};base64,{base64_data}'\n",
    "\n",
    "\n",
    "def image_to_svg(base_path, img_name):\n",
    "    BRAIN_OVERLAY_PNG_PATH = base_path + img_name\n",
    "    print('0')\n",
    "    overlay_img = pixels2svg(BRAIN_OVERLAY_PNG_PATH)\n",
    "    final_img = Drawing(overlay_img.width, overlay_img.height)\n",
    "    final_img.add(Image(href=file_to_base64_html(BRAIN_PNG_PATH),\n",
    "                        size=(\"100%\", \"100%\")))\n",
    "    print('1')\n",
    "    # add some custom style to the output SVG shapes\n",
    "    customized_shapes = []\n",
    "    for element in overlay_img.elements:\n",
    "        # by default pixels2svg groups shapes of same color inside <g> elements\n",
    "        if element.elementname == 'g':\n",
    "            for shape in element.elements:\n",
    "                shape['fill-opacity'] = 0.1\n",
    "                shape['stroke'] = shape['fill']\n",
    "                shape['stroke-width'] = 3\n",
    "                customized_shapes.append(shape)\n",
    "    print('2')\n",
    "    # we'll use clip masks to make sure contours are only drawn on the inside\n",
    "    shape_clip_paths = []\n",
    "    for shape in customized_shapes:\n",
    "        clip_path = ClipPath(id=shape['id'] + '_mask',\n",
    "                             clipPathUnits='userSpaceOnUse')\n",
    "        clip_path.add(shape)\n",
    "        shape_clip_paths.append(clip_path)\n",
    "    print('3')\n",
    "    # add the clip paths to the <defs> tag\n",
    "    for element in final_img.elements:\n",
    "        if element.elementname == 'defs':\n",
    "            for clip_path in shape_clip_paths:\n",
    "                element.add(clip_path)\n",
    "\n",
    "    # add the customized shapes to the svg (no need to re-group them)\n",
    "    for shape, clip_path in zip(customized_shapes, shape_clip_paths):\n",
    "        shape['clip-path'] = f'url(#{clip_path[\"id\"]})'\n",
    "        final_img.add(shape)\n",
    "\n",
    "    print(os.path.join(os.path.dirname(BRAIN_PNG_PATH), 'brain_overlay.svg'))\n",
    "    final_img.save_to_path(os.path.join(os.path.dirname(BRAIN_PNG_PATH), 'brain_overlay.svg'))\n",
    "    #print()\n",
    "ass_dir = 'youtube/output_frames/Easy Falling Leaves - Adobe After Effects tutorial/'\n",
    "base_dir = 'youtube/output_frames/'\n",
    "test_dir = '50_Best_Motion_Logos__Cool_Logo_Animations__Adobe_Creative_Cloud/'\n",
    "\n",
    "source_dir = base_dir + test_dir\n",
    "list_img_to_svg = os.listdir(source_dir)\n",
    "print ('img count ', len(list_img_to_svg))\n",
    "shit_dir = os.listdir(ass_dir)\n",
    "#i = len(shit_dir)\n",
    "print(i)\n",
    "# while i > 0:\n",
    "#     i -= 100\n",
    "#     print(shit_dir[i])\n",
    "#     image_to_svg(shit_dir[i])\n",
    "#os.listdir(shit_dir)\n",
    "# for i, shit in enumerate():\n",
    "#      image_to_svg(source_dir + shit, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6eec75-6290-4173-b59d-b3303488293e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install git+git://github.com/ValentinFrancois/pixels2svg#egg=pixels2svg\n",
    "\n",
    "#\n",
    "#! pip search pixels2svg\n",
    "\n",
    "#! pip install pixels2svg\n",
    "\n",
    "#import pixels2svg\n",
    "#dir(pixels2svg)\n",
    "\n",
    "\n",
    "#pixels2svg\n",
    "#pixels2svg.__version__\n",
    "#!  pip install --upgrade pixels2svg\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56e291ae-f88c-443d-98cd-6a6e41d176cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prewitt_edge_detection(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply horizontal Prewitt kernel\n",
    "    kernel_x = np.array([[-1, 0, 1],\n",
    "                         [-1, 0, 1],\n",
    "                         [-1, 0, 1]])\n",
    "    horizontal_edges = cv2.filter2D(gray_image, -1, kernel_x)\n",
    "    \n",
    "    # Apply vertical Prewitt kernel\n",
    "    kernel_y = np.array([[-1, -1, -1],\n",
    "                         [0, 0, 0],\n",
    "                         [1, 1, 1]])\n",
    "    vertical_edges = cv2.filter2D(gray_image, -1, kernel_y)    \n",
    "    # Ensure both arrays have the same data type\n",
    "    horizontal_edges = np.float32(horizontal_edges)\n",
    "    vertical_edges = np.float32(vertical_edges)\n",
    "    \n",
    "    # Compute gradient magnitude\n",
    "    gradient_magnitude = cv2.magnitude(horizontal_edges, vertical_edges)\n",
    "    \n",
    "    # Optional: Apply thresholding to highlight edges\n",
    "    threshold = 50\n",
    "    _, edges = cv2.threshold(gradient_magnitude, threshold, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "    # Read the input image\n",
    "    image = cv2.imread('flower.jpg')\n",
    "    \n",
    "    # Apply Prewitt edge detection\n",
    "    edges = prewitt_edge_detection(image)\n",
    "    \n",
    "    # Plotting both images using subplots\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Original Image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Detected Edges\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(edges, cmap='gray')\n",
    "    plt.title('Prewitt Edge Detection')\n",
    "    plt.axis('off')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480a51f-ead4-41bc-9e47-32d11a64d01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img count  7068\n",
      "38541\n",
      "frame_10127.jpg\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "\n",
    "#from examples.base import SPACESHIPS_PNG_PATH\n",
    "import os\n",
    "from pixels2svg import pixels2svg\n",
    "ass_dir = 'Easy Falling Leaves - Adobe After Effects tutorial/'\n",
    "base_dir = 'youtube/output_frames/'\n",
    "source_dir = base_dir + ass_dir\n",
    "list_img_to_svg = os.listdir(source_dir)\n",
    "print ('img count ', len(list_img_to_svg))\n",
    "shit_dir = os.listdir(ass_dir)\n",
    "i = len(shit_dir)\n",
    "print(i)\n",
    "def main(SPACESHIPS_PNG_PATH):\n",
    "    svg_img = pixels2svg(SPACESHIPS_PNG_PATH,\n",
    "                         remove_background=True)\n",
    "    output_path = path.join(path.dirname(SPACESHIPS_PNG_PATH),\n",
    "                            'spaceships.svg')\n",
    "    svg_img.save_to_path(output_path)\n",
    "# while i > 0:\n",
    "#     i -= 100\n",
    "#     print(shit_dir[i])\n",
    "#     main(ass_dir + shit_dir[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b972695-754e-4787-bfe5-c428a5141803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "print(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7f004cd-d143-481a-9f32-5ac5ff1c5b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtube/output_frames/Easy Falling Leaves - Adobe After Effects tutorial/frame_00605.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def pixelate_image(image_path, output_path, pixel_size):\n",
    "    \"\"\"\n",
    "    Apply a pixelation effect to an image and save the result.\n",
    "    \n",
    "    :param image_path: Path to the input image\n",
    "    :param output_path: Path to save the pixelated image\n",
    "    :param pixel_size: Size of the pixels in the pixelated image\n",
    "    \"\"\"\n",
    "    # Open the image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Calculate the size of the reduced image\n",
    "    small_img_size = (img.width // pixel_size, img.height // pixel_size)\n",
    "    \n",
    "    # Resize down\n",
    "    small_img = img.resize(small_img_size, resample=Image.NEAREST)\n",
    "    \n",
    "    # Resize up to original size\n",
    "    pixelated_img = small_img.resize(img.size, Image.NEAREST)\n",
    "    \n",
    "    # Save the result\n",
    "    pixelated_img.save(output_path)\n",
    "    #pixelated_img.show()\n",
    "    return pixelated_img\n",
    "\n",
    "\n",
    "\n",
    "# Paths to input and output images\n",
    "#https://files.hashirama.blog/youtube/output_frames/Easy%20Falling%20Leaves%20-%20Adobe%20After%20Effects%20tutorial/frame_00097.jpg\n",
    "src = 'youtube/output_frames/Easy Falling Leaves - Adobe After Effects tutorial/'\n",
    "\n",
    "# ass_dir = 'Easy Falling Leaves - Adobe After Effects tutorial/'\n",
    "# base_dir = 'youtube/output_frames/'\n",
    "#source_dir = base_dir + ass_dir\n",
    "img_name = 'frame_00605.jpg'\n",
    "input_image_path = src + img_name  # Original image\n",
    "output_image_path = \"./pixelated_image.png\"  # Pixelated image output\n",
    "\n",
    "# Pixel size for the effect\n",
    "pixel_size = 16\n",
    "\n",
    "# Apply pixelation effect\n",
    "print(input_image_path)\n",
    "pixelated_img = pixelate_image(input_image_path, output_image_path, pixel_size)\n",
    "#display(pixelated_img)\n",
    "main(output_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54ecf2b9-28f3-41c8-94f7-dc0c546356cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp spaceships.svg youtube/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fd226da-b0de-4d01-8ec8-27612bb5c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, SVG\n",
    "\n",
    "# Path to your SVG file\n",
    "svg_file_path = './spaceships.svg'\n",
    "\n",
    "# Display the SVG file\n",
    "#display(SVG(filename=svg_file_path))\n",
    "\n",
    "\n",
    "#render the alien space ship from 2022-2023\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17e9ae9a-d8f5-4a80-8b07-7b0da3c50493",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 11) (2526107722.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    'https://apenwarr.ca/log/?m=202304,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 11)\n"
     ]
    }
   ],
   "source": [
    "blogs = [\n",
    "    'https://www.latent.space/',\n",
    "    'https://ai.meta.com/sam2/',\n",
    "    'https://www.latent.space/',\n",
    "    'https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg',\n",
    "    'https://www.youtube.com/computerphile',\n",
    "    'https://link.springer.com/article/10.1007/s40747-021-00428-4',\n",
    "    'https://macwright.com/',\n",
    "    'https://richzhang.github.io/colorization/',\n",
    "    'https://macwright.com/2024/07/30/reverse-engineer-a-day',\n",
    "    'https://apenwarr.ca/log/?m=202304,\n",
    "]\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_folder):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    if not video.isOpened():\n",
    "        print(f\"Error: Could not open video: {video_path}\")\n",
    "        return\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        # Read the next frame from the video\n",
    "        success, frame = video.read()\n",
    "\n",
    "        # If there are no more frames, break the loop\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Save the frame as an image file\n",
    "        frame_path = os.path.join(output_folder, f\"frame_{frame_count:05d}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "\n",
    "        # Increment the frame count\n",
    "        frame_count += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    video.release()\n",
    "    print(f\"Extracted {frame_count} frames to {output_folder}\")\n",
    "\n",
    "# Example usage\n",
    "mp4_files = os.listdir('youtube/mp4_content/')\n",
    "output_dir = 'youtube/output_frames/'  # Directory to save the frames\n",
    "\n",
    "def divide_frames():\n",
    "    for video_file in mp4_files:\n",
    "        pass\n",
    "        video_path = os.path.join('youtube/mp4_content', video_file)  # Full path to the video\n",
    "        print(f\"Processing video: {video_path}\")\n",
    "        \n",
    "        if video_file.endswith('.mp4'):  # Ensure we're only processing mp4 files\n",
    "            extract_frames(video_path, os.path.join(output_dir, video_file.replace(\".mp4\", \"\")))\n",
    "        else:\n",
    "            print(f\"Skipping non-video file: {video_file}\")\n",
    "    #1 download 1000 videos\n",
    "#2. split into frames\n",
    "#3. process frames\n",
    "#3a segment -> convert masked areas to SVG -> triagnluate svg to match dcolors\n",
    "#3b label frames\n",
    "#3c high entropy frames - seam carving\n",
    "#3d replicate / apis to try stuff -> move to local to optimize + make chaepr\n",
    "#4. join svg frames into lottie\n",
    "# SAVE_PATH = \"./youtube/mp4_content\" \n",
    "# print(SAVE_PATH)\n",
    "# import os\n",
    "# #! mkdir -p youtube/mp4_content\n",
    "# os.listdir('youtube/mp4_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af60f0-df11-4203-b863-e304091015a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pytubefix import Playlist\n",
    "# from pytubefix import YouTube \n",
    "\n",
    "# from pytubefix.cli import on_progress\n",
    "# SAVE_PATH = \"./youtube\" \n",
    "\n",
    "# brown_blue_math_playlists = [\n",
    "#     'https://www.youtube.com/watch?v=d-o3eB9sfls&list=PLZHQObOWTQDPHP40bzkb0TKLRPwQGAoC-',\n",
    "#     'https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab',\n",
    "#     'https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr',\n",
    "#     'https://www.youtube.com/watch?v=r6sGWTCMz2k&list=PLZHQObOWTQDN52m7Y21ePrTbvXkPaWVSg',\n",
    "#     'https://www.youtube.com/watch?v=OkmNXy7er84&list=PLZHQObOWTQDMXMi3bUMThGdYqos36X_lA',\n",
    "#     'https://www.youtube.com/watch?v=8rrHTtUzyZA&list=PLZHQObOWTQDMp_VZelDYjka8tnXNpXhzJ',\n",
    "# ]\n",
    "\n",
    "# SAVE_PATH = \"./youtube/mp4_content/\"\n",
    "\n",
    "# def download_yt(link):\n",
    "#     SAVE_PATH = \"./youtube/mp4_content/\" \n",
    "#     try: \n",
    "#         # object creation using YouTube \n",
    "#         yt = YouTube(link) \n",
    "#     except Exception as e: \n",
    "#         # to handle exception and print error\n",
    "#         print(f\"Connection Error for {link}: {e}\")\n",
    "#         return  # Exit the function if there's an error\n",
    "\n",
    "#     # Check if yt is defined\n",
    "#     if yt is not None:\n",
    "#         # Get all streams and filter for mp4 files\n",
    "#         mp4_streams = yt.streams.filter(file_extension='mp4').all()\n",
    "\n",
    "#         # Check if there are any mp4 streams\n",
    "#         if mp4_streams:\n",
    "#             # get the video with the highest resolution\n",
    "#             d_video = mp4_streams[0]\n",
    "            \n",
    "#             try: \n",
    "#                 # downloading the video \n",
    "#                 d_video.download(output_path=SAVE_PATH)\n",
    "#                 print('Video downloaded successfully!')\n",
    "#             except Exception as e: \n",
    "#                 print(f\"Some Error occurred while downloading {link}: {e}\")\n",
    "#         else:\n",
    "#             print(f\"No mp4 streams available for {link}\")\n",
    "#     else:\n",
    "#         print(f\"yt object not created for {link}\")\n",
    "\n",
    "# youtube_86_motiongraphics_playlist = ['https://www.youtube.com/playlist?list=PL79TBmLa4lTTq9w7AYjaLo9RSXkzrihVY']\n",
    "\n",
    "# for playlist in (brown_blue_math_playlists + youtube_86_motiongraphics_playlist): \n",
    "#     pl = Playlist(playlist)\n",
    "#     for url in pl.video_urls:\n",
    "#         print(url)\n",
    "#         download_yt(url)\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pytubefix import Playlist, YouTube, exceptions\n",
    "from pytubefix.cli import on_progress\n",
    "\n",
    "SAVE_PATH = \"./youtube/mp4_content/\"\n",
    "\n",
    "# List of playlists to download\n",
    "brown_blue_math_playlists = [\n",
    "    'https://www.youtube.com/watch?v=d-o3eB9sfls&list=PLZHQObOWTQDPHP40bzkb0TKLRPwQGAoC-',\n",
    "    'https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab',\n",
    "    'https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr',\n",
    "    'https://www.youtube.com/watch?v=r6sGWTCMz2k&list=PLZHQObOWTQDN52m7Y21ePrTbvXkPaWVSg',\n",
    "    'https://www.youtube.com/watch?v=OkmNXy7er84&list=PLZHQObOWTQDMXMi3bUMThGdYqos36X_lA',\n",
    "    'https://www.youtube.com/watch?v=8rrHTtUzyZA&list=PLZHQObOWTQDMp_VZelDYjka8tnXNpXhzJ',\n",
    "]\n",
    "\n",
    "youtube_86_motiongraphics_playlist = ['https://www.youtube.com/playlist?list=PL79TBmLa4lTTq9w7AYjaLo9RSXkzrihVY']\n",
    "\n",
    "# Combined list of playlists\n",
    "playlists = brown_blue_math_playlists + youtube_86_motiongraphics_playlist\n",
    "\n",
    "def video_exists(video_title):\n",
    "    \"\"\"Check if the video file already exists in the SAVE_PATH directory.\"\"\"\n",
    "    # Create a valid filename by replacing illegal characters\n",
    "    filename = f\"{video_title}.mp4\".replace('/', '-').replace('\\\\', '-').replace('|', '-').replace('?', '')\n",
    "    return os.path.exists(os.path.join(SAVE_PATH, filename))\n",
    "\n",
    "def download_yt(link):\n",
    "    \"\"\"Download the YouTube video from the provided link if it doesn't already exist.\"\"\"\n",
    "    try: \n",
    "        # Create YouTube object\n",
    "        yt = YouTube(link)\n",
    "        video_title = yt.title\n",
    "\n",
    "        if video_exists(video_title):\n",
    "            print(f\"Video '{video_title}' already exists. Skipping download.\")\n",
    "            return\n",
    "\n",
    "        # Filter for mp4 streams\n",
    "        mp4_streams = yt.streams.filter(file_extension='mp4')\n",
    "        \n",
    "        # Get the highest resolution video\n",
    "        if mp4_streams:\n",
    "            d_video = mp4_streams.order_by('resolution').desc().first()\n",
    "            \n",
    "            try: \n",
    "                # Download the video \n",
    "                d_video.download(output_path=SAVE_PATH)\n",
    "                print(f\"Video '{video_title}' downloaded successfully!\")\n",
    "            except Exception as e: \n",
    "                print(f\"Error occurred while downloading '{video_title}': {e}\")\n",
    "        else:\n",
    "            print(f\"No mp4 streams available for '{video_title}'\")\n",
    "            \n",
    "    except exceptions.VideoUnavailable:\n",
    "        print(f\"Video {link} is unavailable. Skipping.\")\n",
    "    except exceptions.RegexMatchError:\n",
    "        print(f\"Invalid video URL: {link}. Skipping.\")\n",
    "    except Exception as e: \n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        # Rate limiting to avoid triggering warnings\n",
    "        time.sleep(2)  # Wait for 2 seconds before processing the next video\n",
    "\n",
    "# for playlist in playlists: \n",
    "#     pl = Playlist(playlist)\n",
    "#     for url in pl.video_urls:\n",
    "#         print(f\"Processing {url}\")\n",
    "#         download_yt(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db6459e4-4b50-4148-9f21-1e5f16979c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install watchdog\n",
    "# hard + vlauable + fun (focus so single pointedly - evertything else fades away)\n",
    "# bookish like crodelia - yoga/tech like olvia or abby, \n",
    "# you know what you are now a bit - so you wont have any issues \n",
    "# look dont matter - behavior, leadership, self-ctornol, happy-mind, lifestyle = these are the foundation of a  happy relationship\n",
    "brown_blue_math_playists = [\n",
    "    'https://www.youtube.com/watch?v=d-o3eB9sfls&list=PLZHQObOWTQDPHP40bzkb0TKLRPwQGAoC-',\n",
    "'https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab',\n",
    "'https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr',\n",
    "'https://www.youtube.com/watch?v=r6sGWTCMz2k&list=PLZHQObOWTQDN52m7Y21ePrTbvXkPaWVSg',\n",
    "'https://www.youtube.com/watch?v=OkmNXy7er84&list=PLZHQObOWTQDMXMi3bUMThGdYqos36X_lA',\n",
    "'https://www.youtube.com/watch?v=8rrHTtUzyZA&list=PLZHQObOWTQDMp_VZelDYjka8tnXNpXhzJ',\n",
    "]\n",
    "\n",
    "#youtube_86_motiongraphics_playlist  = 'https://www.youtube.com/playlist?list=PL79TBmLa4lTTq9w7AYjaLo9RSXkzrihVY'\n",
    "#xml_pase = 'https://myanimelist.net/rss.php?type=rw&u=rimmjob'\n",
    "\n",
    "# first -> dice ~100 papers, anime, youtube into frames\n",
    "# -> use vision transformer to label them\n",
    "\n",
    "\n",
    "# use 100 blogs -> use flux\n",
    "# get 100 prompts from 100 research papers \n",
    "# get 100 prompts from 100 anime \n",
    "# get 100 prompts from 100 youtube\n",
    "# prompts = label\n",
    "# prompt to gif + lottie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f092a8e5-c8de-423e-9582-b1ac58ce43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytubefix import YouTube \n",
    "from pytubefix.cli import on_progress\n",
    "from pytubefix.innertube import _default_clients\n",
    "\n",
    "#apm  - effective  - \n",
    "\n",
    "# art of tennis\n",
    "\n",
    "SAVE_PATH = \"./youtube\" #to_do \n",
    "apple = 'https://www.youtube.com/watch?v=cwCMV7LF6S4'\n",
    "#apple = 'https://youtu.be/cwCMV7LF6S4?si=BvAYyjbT4dkoiM1E'\n",
    "todoroki = 'https://www.youtube.com/watch?v=wpOhT35YtWg'\n",
    "light = 'https://www.youtube.com/watch?v=fiZhLla-2wA'\n",
    "flower = 'https://www.youtube.com/watch?v=QSKBJvq8FDI&pp=ygURb3VyYW4gcGFpbnRpbmcgMjQ%3D'\n",
    "logos = 'https://www.youtube.com/watch?v=1tj7Y3PR16s'\n",
    "names = 'logos flower light todoroki apple'.split(' ')\n",
    "links = [logos, flower, light, todoroki, apple] \n",
    "# https://github.com/JuanBindez/pytubefix\n",
    "#os.chdir(r\"./youtube\")\n",
    "def download_yt(name, link):\n",
    "    try: \n",
    "        # object creation using YouTube \n",
    "        yt = YouTube(link) \n",
    "    except: \n",
    "        #to handle exception \n",
    "        print(\"Connection Error\") \n",
    "    \n",
    "    # Get all streams and filter for mp4 files\n",
    "    mp4_streams = yt.streams.filter(file_extension='mp4'\n",
    "                                    #, on_progress_callback = on_progress\n",
    "                                   ).all()\n",
    "    \n",
    "    # get the video with the highest resolution\n",
    "    d_video = mp4_streams[0]\n",
    "    #print(mp4_streams)\n",
    "    try: \n",
    "        # downloading the video \n",
    "        d_video.download(output_path=SAVE_PATH)\n",
    "        print('Video downloaded successfully!')\n",
    "    except: \n",
    "        print(\"Some Error!\")\n",
    "\n",
    "# for i, link in enumerate(links): \n",
    "#     download_yt(names[i], link)\n",
    "#download_yt('apple', apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6afbf100-ea47-480f-b2b8-a1edc587b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "def swap_dir():\n",
    "    print(os.listdir('.'))\n",
    "    os.chdir(r\"./youtube\")\n",
    "    print(os.listdir('.'))\n",
    "    os.chdir(r\"../\")\n",
    "    print(os.listdir('.'))\n",
    "\n",
    "#os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d347ced-8aa4-4388-aac5-b18ff0fca443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtube/output_frames\n"
     ]
    }
   ],
   "source": [
    "# ! ls youtube\n",
    "# ! mkdir -p youtube/output_frames\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_folder):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not video.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        # Read the next frame from the video\n",
    "        success, frame = video.read()\n",
    "\n",
    "        # If there are no more frames, break the loop\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Save the frame as an image file\n",
    "        frame_path = os.path.join(output_folder, f\"frame_{frame_count:05d}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "\n",
    "        # Increment the frame count\n",
    "        frame_count += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    video.release()\n",
    "    print(f\"Extracted {frame_count} frames to {output_folder}\")\n",
    "\n",
    "# Example usage\n",
    "mp4 = os.listdir('youtube/')\n",
    "output_dir = 'youtube/output_frames/'    # Directory to save the frames\n",
    "\n",
    "for video_file in mp4:\n",
    "    video_file = 'youtube/'+video_file\n",
    "    print(video_file)\n",
    "    # try:\n",
    "    #     os.mkdir()\n",
    "    #     print(f\"Directory '{directory_name}' created successfully.\")\n",
    "    # except FileExistsError:\n",
    "    #     print(f\"Directory '{directory_name}' already exists.\")\n",
    "    #extract_frames(video_file, output_dir + video_file)\n",
    "    pass\n",
    "\n",
    "# get each frame -> process frame -> convert to svg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9069962a-8f55-4938-b875-0abeec3dc459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_frames(video_path, output_folder):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(str(video_path))  # Convert Path object to string\n",
    "    \n",
    "    if not video.isOpened():\n",
    "        print(f\"Error: Could not open video: {video_path}\")\n",
    "        return\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        # Read the next frame from the video\n",
    "        success, frame = video.read()\n",
    "\n",
    "        # If there are no more frames, break the loop\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Save the frame as an image file\n",
    "        frame_path = output_folder / f\"frame_{frame_count:05d}.jpg\"\n",
    "        cv2.imwrite(str(frame_path), frame)\n",
    "\n",
    "        # Increment the frame count\n",
    "        frame_count += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    video.release()\n",
    "    print(f\"Extracted {frame_count} frames to {output_folder}\")\n",
    "\n",
    "# Example usage\n",
    "mp4_files = Path('.').glob('*.mp4')  # Use Pathlib to get all .mp4 files in the folder\n",
    "output_dir = Path('youtube/output_frames')  # Directory to save the frames\n",
    "\n",
    "#mp4_files = [\"output_reencoded.mp4\"]\n",
    "#mp4_files = [\"./shit.mp4\"]\n",
    "# for video_file in mp4_files:\n",
    "#     pass\n",
    "#     video_path = video_file\n",
    "#     print(f\"Processing video: {video_path}\")\n",
    "# #    video_output_folder = './'\n",
    "#     video_output_folder = output_dir / video_file.stem  # Create subfolder based on video file name\n",
    "#     extract_frames(video_path, video_output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91c6ba5-1ed7-4ecf-bfe0-19f7c5d0a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.listdir('./youtube/output_frames/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fb6f2ab-951d-4300-8a92-34eac3800b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ls youtube/\n",
    "#! rm *.mp4\n",
    "#! mv youtube/*.mp4 .\n",
    "#! rmdir youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bad97330-b500-4879-b2c9-c2c01cdb248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ffmpeg -i \"youtube/Deku_vs_Todoroki_My_Hero_Academia.mp4\" -vcodec libx264 -acodec aac \"output.mp4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e90b5e0-4742-4ff3-a188-86b8e8fcd666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Linker flags (Release):      -Wl,--exclude-libs,libippicv.a -Wl,--exclude-libs,libippiw.a -L/ffmpeg_build/lib  -Wl,--gc-sections -Wl,--as-needed -Wl,--no-undefined  \n",
      "    Linker flags (Debug):        -Wl,--exclude-libs,libippicv.a -Wl,--exclude-libs,libippiw.a -L/ffmpeg_build/lib  -Wl,--gc-sections -Wl,--as-needed -Wl,--no-undefined  \n",
      "    FFMPEG:                      YES\n"
     ]
    }
   ],
   "source": [
    "# ! sudo apt update\n",
    "# ! sudo apt install openh264\n",
    "#import cv2\n",
    "#print(cv2.getBuildInformation())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62050bae-290b-4fcf-879c-aaf0a9e84051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.roboflow.com/edge-detection/\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def blah():\n",
    "    # Load the image\n",
    "    image_path = 'flower.jpg'  # Replace with your image path\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 1.4)\n",
    "    \n",
    "    # Apply Canny edge detector\n",
    "    edges = cv2.Canny(blurred_image, 100, 200)\n",
    "    \n",
    "    # Display the result\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Edge-detected image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(edges, cmap='gray')\n",
    "    plt.title('Canny Edge Detection')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #https://blog.roboflow.com/edge-detection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "261c110b-9315-407d-b58e-97a5289198b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "mp4_files = Path('./').glob('*.mp4')\n",
    "\n",
    "def rename_files():\n",
    "    for video_file in mp4_files:\n",
    "        new_name = video_file.stem.replace(' ', '_') + video_file.suffix\n",
    "        new_path = video_file.with_name(new_name)\n",
    "        print(video_file, new_name, new_path)\n",
    "    \n",
    "        video_file.rename(new_path)\n",
    "        print(f'Renamed {video_file} to {new_path}')\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the directory containing subdirectories you want to rename\n",
    "def rename_dirs():\n",
    "    mp4_dirs = Path('youtube/output_frames/')\n",
    "    \n",
    "    # Iterate over all subdirectories in the given path\n",
    "    for directory in mp4_dirs.iterdir():\n",
    "        if directory.is_dir():  # Ensure we are only renaming directories\n",
    "            # Replace spaces with underscores and rename the directory\n",
    "            new_name = directory.stem.replace(' ', '_') + directory.suffix\n",
    "            new_path = directory.with_name(new_name)\n",
    "            \n",
    "            # Rename the directory\n",
    "            directory.rename(new_path)\n",
    "            \n",
    "            print(f'Renamed directory {directory} to {new_path}')\n",
    "#rename_dirs()\n",
    "#from sam2.build_sam import build_sam2\n",
    "#from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77978ddb-612c-4809-a46f-18d3143ed4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frame_00428.jpg',\n",
       " 'frame_01268.jpg',\n",
       " 'frame_01609.jpg',\n",
       " 'frame_00049.jpg',\n",
       " 'frame_00098.jpg']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.listdir('youtube/output_frames/ohshc__Finding_out_Haruhi_is_a_girl/')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38f9fd84-8a69-4543-812e-f8393a772c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "opening ../homelab-status-page/static/webcam.jpg\n",
      "opening ../homelab-status-page/static/webcam.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../homelab-status-page/static/webcam.jpg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28144490-379c-4d35-b5f1-718c965d8d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#str(Path(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7200ccc3-4803-4437-b639-26f7adb6c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sam2.build_sam import build_sam2\n",
    "# from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "from pathlib import Path\n",
    "\n",
    "def mask_generate(url=\"../homelab-status-page/static/webcam.jpg\"):\n",
    "    import os\n",
    "    # if using Apple MPS, fall back to CPU for unsupported ops\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    \n",
    "    # select the device for computation\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"using device: {device}\")\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        # use bfloat16 for the entire notebook\n",
    "        torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "        if torch.cuda.get_device_properties(0).major >= 8:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "    elif device.type == \"mps\":\n",
    "        print(\n",
    "            \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "            \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "            \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "        )\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    \n",
    "    def show_anns(anns, borders=True):\n",
    "        if len(anns) == 0:\n",
    "            return\n",
    "        sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "        ax = plt.gca()\n",
    "        ax.set_autoscale_on(False)\n",
    "    \n",
    "        img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "        img[:, :, 3] = 0\n",
    "        for ann in sorted_anns:\n",
    "            m = ann['segmentation']\n",
    "            color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
    "            img[m] = color_mask \n",
    "            if borders:\n",
    "                import cv2\n",
    "                contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "                # Try to smooth contours\n",
    "                contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "                cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1) \n",
    "    \n",
    "        ax.imshow(img)\n",
    "    file_name = url\n",
    "    \n",
    "    output_dir = Path('./youtube/semseg/ohshc__Finding_out_Haruhi_is_a_girl/')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    output_file_path = output_dir / file_name\n",
    "    print(\"going to save file at \" + str(output_file_path))\n",
    "\n",
    "    # print(\"opening \" + url)\n",
    "    # #print(output)\n",
    "    # print(\"opening \" + url)\n",
    "\n",
    "    #return url\n",
    "    image = Image.open('./youtube/output_frames/ohshc__Finding_out_Haruhi_is_a_girl/' + url)\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    from sam2.build_sam import build_sam2\n",
    "    from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "    \n",
    "    sam2_checkpoint = \"./sam2_hiera_tiny.pt\"\n",
    "    model_cfg = \"sam2_hiera_t.yaml\"\n",
    "    \n",
    "    sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)\n",
    "    \n",
    "    mask_generator = SAM2AutomaticMaskGenerator(sam2)\n",
    "    \n",
    "    masks = mask_generator.generate(image)\n",
    "    \n",
    "    print(len(masks))\n",
    "    print(masks[0].keys())\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(image)\n",
    "    show_anns(masks)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    mask_generator_2 = SAM2AutomaticMaskGenerator(\n",
    "        model=sam2,\n",
    "        points_per_side=64,\n",
    "        points_per_batch=128,\n",
    "        pred_iou_thresh=0.7,\n",
    "        stability_score_thresh=0.92,\n",
    "        stability_score_offset=0.7,\n",
    "        crop_n_layers=1,\n",
    "        box_nms_thresh=0.7,\n",
    "        crop_n_points_downscale_factor=2,\n",
    "        min_mask_region_area=25.0,\n",
    "        use_m2m=True,\n",
    "    )\n",
    "    \n",
    "    masks2 = mask_generator_2.generate(image)\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(image)\n",
    "    show_anns(masks2)\n",
    "    plt.axis('off')\n",
    "    #plt.show()\n",
    "    plt.savefig(output_file_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()  # Close the plot to free up memory\n",
    "    output_file_path_2 = output_dir / (file_name.split('.')[0] + '_2.png')\n",
    "    print(f\"Segmented image saved at: {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7783196c-80f3-4cba-b812-5e6d5984900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f5fc104-d3b7-452c-8726-d88d26f3c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! micromamba info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9da597f4-045d-417b-a090-e5bd1654f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10bdfe67-cb9e-428c-a744-152b76a75170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process images = \n",
    "\n",
    "# label frames -> dsicard non needed\n",
    "# semantic segemenatiaon -> discard pixels not neded \n",
    "# someconvert from bitmap to svg -> triangulate using earcut \n",
    "\n",
    "# Lightly adapted from https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\n",
    "import os\n",
    "\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "# for image in os.listdir('youtube/output_frames/ohshc__Finding_out_Haruhi_is_a_girl/')[:5]:\n",
    "  \n",
    "#     #img = PILImage.open(f\"output_frames/ohshc__Finding_out_Haruhi_is_a_girl/{image}\")\n",
    "#     print(image)\n",
    "#     mask_generate(image)\n",
    "#     #mask_generate(f\"output_frames/ohshc__Finding_out_Haruhi_is_a_girl/{image}\")\n",
    "#     #display(img)\n",
    "#     time.sleep(3)\n",
    "\n",
    "#test_display = 'output_frames/ohshc__Finding_out_Haruhi_is_a_girl/frame_00428.jpg'\n",
    "# Open and display the image\n",
    "#img = PILImage.open(test_display)\n",
    "#display(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b21f47-d520-4300-a124-92ce067c0ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: svgwrite in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (1.4.3)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install opencv-python svgwrite\n",
    "# ! sudo apt install potrace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c82e0688-dadd-450f-bc74-4281ac543c54",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 56\u001b[0m\n\u001b[1;32m     52\u001b[0m     dwg\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVG saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msvg_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myoutube/semseg/ohshc__Finding_out_Haruhi_is_a_girl/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#os.listdir('youtube/semseg/ohshc__Finding_out_Haruhi_is_a_girl/')\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# for mask in masks: \u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#     masks_to_svg(mask)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import svgwrite\n",
    "\n",
    "# Load the image using OpenCV\n",
    "\n",
    "def masks_to_svg(image_path):\n",
    "    #image_path = '/mnt/data/image.png'  # Path to your image\n",
    "    #return \n",
    "    print(image_path)\n",
    "    image = cv2.imread(f\"youtube/semseg/ohshc__Finding_out_Haruhi_is_a_girl/{image_path}\")\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply edge detection or thresholding to simplify the image\n",
    "    ret, thresh = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours in the image\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create an SVG drawing\n",
    "    svg_filename = 'youtube/svg/' + image_path.replace('.jpg', '.svg')\n",
    "    dwg = svgwrite.Drawing(svg_filename, profile='tiny')\n",
    "    \n",
    "    # Define the size of the SVG canvas\n",
    "    height, width = image.shape[:2]\n",
    "    dwg.viewbox(width=width, height=height)\n",
    "    \n",
    "    # Iterate through each contour and convert it to an SVG path\n",
    "    for contour in contours:\n",
    "        # Simplify the contour\n",
    "        approx = cv2.approxPolyDP(contour, 0.01 * cv2.arcLength(contour, True), True)\n",
    "        \n",
    "        # Create SVG path data\n",
    "        path_data = []\n",
    "        for point in approx:\n",
    "            x, y = point[0]\n",
    "            path_data.append(f'L{x},{y}')\n",
    "        \n",
    "        # Start path from the first point\n",
    "        if path_data:\n",
    "            path_data[0] = path_data[0].replace('L', 'M')  # Replace the first 'L' with 'M'\n",
    "        \n",
    "        # Join path data into a single string and close the path\n",
    "        path_str = ' '.join(path_data) + ' Z'\n",
    "        \n",
    "        # Add the path to the SVG\n",
    "        dwg.add(dwg.path(d=path_str, fill='none', stroke='black', stroke_width=1))\n",
    "    \n",
    "    # Save the SVG file\n",
    "    dwg.save()\n",
    "    \n",
    "    print(f\"SVG saved as {svg_filename}\")\n",
    "\n",
    "masks = os.listdir('youtube/semseg/ohshc__Finding_out_Haruhi_is_a_girl/')\n",
    "\n",
    "#os.listdir('youtube/semseg/ohshc__Finding_out_Haruhi_is_a_girl/')\n",
    "# for mask in masks: \n",
    "#     masks_to_svg(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07e79ea8-4aac-49ed-93c5-90b3bba463d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masks_to_svg(image_path):\n",
    "    # Check if the file exists and can be opened\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: File {image_path} not found.\")\n",
    "        return\n",
    "    \n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Ensure the image was loaded successfully\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image {image_path}.\")\n",
    "        return\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian Blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Adjust thresholding to detect stronger edges\n",
    "    ret, thresh = cv2.threshold(blurred, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours in the image\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filter out small contours by area\n",
    "    min_contour_area = 100  # You can adjust this value to remove smaller details\n",
    "    contours = [c for c in contours if cv2.contourArea(c) > min_contour_area]\n",
    "\n",
    "    # Create an SVG drawing\n",
    "    svg_filename = 'output_image_improved.svg'\n",
    "    dwg = svgwrite.Drawing(svg_filename, profile='tiny')\n",
    "\n",
    "    # Define the size of the SVG canvas\n",
    "    height, width = image.shape[:2]\n",
    "    dwg.viewbox(width=width, height=height)\n",
    "\n",
    "    # Iterate through each contour and convert it to an SVG path\n",
    "    for contour in contours:\n",
    "        # Simplify the contour\n",
    "        approx = cv2.approxPolyDP(contour, 0.01 * cv2.arcLength(contour, True), True)\n",
    "        \n",
    "        # Create SVG path data\n",
    "        path_data = []\n",
    "        for point in approx:\n",
    "            x, y = point[0]\n",
    "            path_data.append(f'L{x},{y}')\n",
    "        \n",
    "        # Start path from the first point\n",
    "        if path_data:\n",
    "            path_data[0] = path_data[0].replace('L', 'M')  # Replace the first 'L' with 'M'\n",
    "        \n",
    "        # Join path data into a single string and close the path\n",
    "        path_str = ' '.join(path_data) + ' Z'\n",
    "        \n",
    "        # Add the path to the SVG\n",
    "        dwg.add(dwg.path(d=path_str, fill='none', stroke='black', stroke_width=1))\n",
    "\n",
    "    # Save the SVG file\n",
    "    dwg.save()\n",
    "\n",
    "    print(f\"SVG saved as {svg_filename}\")\n",
    "\n",
    "# Example usage\n",
    "test_image = 'youtube/semseg/ohshc__Finding_out_Haruhi_is_a_girl/frame_00049.jpg'\n",
    "#image_path = 'path_to_your_image.png'\n",
    "#masks_to_svg(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb07a95-3d5a-4bea-bd50-77b644afd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "\n",
    "# Path to your SVG file\n",
    "svg_path = \"output_image_improved.svg\"\n",
    "\n",
    "# Display the SVG\n",
    "#display(SVG(filename=svg_path))\n",
    "#https://jupyterlab.readthedocs.io/en/stable/user/extensions.html\n",
    "#https://github.com/jpmorganchase/python-training\n",
    "# https://github.com/observablehq/framework/blob/main/docs/lib/mosaic.md\n",
    "# https://docs.jupyter.org/en/latest/projects/incubator.html\n",
    "# https://www.ycombinator.com/companies/srcbook\n",
    "# https://www.ycombinator.com/companies/neptyne\n",
    "# https://github.com/dformoso/machine-learning-mindmap\n",
    "# https://www.fast.ai/\n",
    "# https://blog.jupyter.org/generative-ai-in-jupyter-3f7174824862\n",
    "# https://github.com/jupyterlab/jupyter-ai\n",
    "# https://app.lottiefiles.com/\n",
    "# https://news.ycombinator.com/item?id=41234424\n",
    "# https://creator.lottiefiles.com/?fileId=538b752c-2972-457d-9d49-64947206479b\n",
    "# https://github.com/fireship-io/animated-svg-demo\n",
    "# https://svg.io/download/2e62e1ed-b403-4137-b2b3-00221fa2994e\n",
    "# https://www.domoai.app/home?_gl=1*xxzt2g*_gcl_au*MTAzNjg0NTQzOS4xNzI2NjM5Nzg0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71f771ae-b263-40df-9790-42c5bcf0468a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ohshc__Finding_out_Haruhi_is_a_girl',\n",
       " '50_Best_Motion_Logos__Cool_Logo_Animations__Adobe_Creative_Cloud',\n",
       " 'Ouran_High_School_Host_Club_-_Kyouyas_Laugh_[English_Dub]',\n",
       " 'Designed_By_Apple_Intention',\n",
       " 'Deku_vs_Todoroki__My_Hero_Academia']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('youtube/output_frames')\n",
    "#os.listdir('youtube/output/ohshc__Finding_out_Haruhi_is_a_girl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e1f3534f-cf3e-4a8a-82d0-72f9d2a8ddff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"output_frames/ohshc__Finding_out_Haruhi_is_a_girl/frame_00428.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import Image\n",
    "test_display = 'output_frames/ohshc__Finding_out_Haruhi_is_a_girl/frame_00428.jpg'\n",
    "# Display image from a URL\n",
    "Image(url=test_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ca594d-d0b3-4a0c-8420-3e2c8a008c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "from IPython.display import display\n",
    "#test_display = 'output_frames/ohshc__Finding_out_Haruhi_is_a_girl/frame_00428.jpg'\n",
    "# Open and display the image\n",
    "#img = PILImage.open(test_display)\n",
    "#display(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "87d3cef6-08c6-4e80-9634-8a2496a8e36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/home/adnan/Videos/youtube': File exists\n"
     ]
    }
   ],
   "source": [
    "# ! ln -s ~/hashirama/services/perception-stack/youtube ~/Videos\n",
    "#! rm \n",
    "# ! ls youtube\n",
    "# ! rm \n",
    "# ! ln -s ~/Videos/youtube ~/hashirama/services/perception-stack/youtube\n",
    "# https://chatgpt.com/c/66ea5156-b760-8013-b7bc-d11a8a520929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0886c0bf-6433-48b8-bff6-43e08ac2ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "#fig, axs = plt.subplots(1, 2)  # Adjust to display more images\n",
    "\n",
    "#first_movie = os.listdir('output_frames/')[0]\n",
    "#! mv youtube/output_frames/* output_frames\n",
    "#frames = os.listdir('output_frames/' + first_movie)\n",
    "\n",
    "#a = f\"output_frames/{first_movie}/{frames[0]}\"\n",
    "#b = f\"output_frames/{first_movie}/{frames[2]}\"\n",
    "\n",
    "#shit = f\"{a}/\"\n",
    "#shit\n",
    "#img1 = mpimg.imread(a)\n",
    "#img2 = mpimg.imread(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f887091-b24a-45e1-a597-e92e2cbcf5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2)  # Adjust to display more images\n",
    "\n",
    "# # Load and display images\n",
    "\n",
    "# img1 = mpimg.imread('path_to_your_image1.jpg')\n",
    "# img2 = mpimg.imread('path_to_your_image2.jpg')\n",
    "\n",
    "# # Plot the images\n",
    "# axs[0].imshow(img1)\n",
    "# axs[0].axis('off')\n",
    "# axs[1].imshow(img2)\n",
    "# axs[1].axis('off')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703548f6-8e9e-4227-85d4-0b58d08349e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_animation(yt_link: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Takes a string and returns a tuple containing the file paths of a GIF and a Lottie file.\n",
    "    \"\"\"\n",
    "    # Logic to create and return GIF and Lottie file paths\n",
    "    return \"path_to_gif.gif\"\n",
    "    return (\"path_to_gif.gif\", \"path_to_lottie.json\")\n",
    "\n",
    "\"ama kakeru ryu no hirameki\"\n",
    "def text_to_gif_lottie(prompt: str):\n",
    "    return 10\n",
    "ouran = 'https://www.youtube.com/watch?v=cwCMV7LF6S4&t=3s'\n",
    "apple = 'https://youtu.be/cwCMV7LF6S4?si=BvAYyjbT4dkoiM1E'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f019e74-8391-45a8-bc01-e79b8b40c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #! pwd\n",
    "\n",
    "# print(cv2.getBuildInformation())\n",
    "\n",
    "\n",
    "# from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "\n",
    "# # Load the pre-trained Vision Transformer model and feature extractor\n",
    "# model_name = 'google/vit-base-patch16-224'  # You can choose a different model as well\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "# model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# # Set model to evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# def preprocess_frame(frame):\n",
    "#     # Convert the frame to PIL Image\n",
    "#     image = Image.fromarray(frame)\n",
    "    \n",
    "#     # Use the feature extractor to preprocess the image\n",
    "#     inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "#     return inputs['pixel_values']\n",
    "\n",
    "\n",
    "\n",
    "# def label_frame(frame):\n",
    "#     # Preprocess the frame\n",
    "#     inputs = preprocess_frame(frame)\n",
    "    \n",
    "#     # Forward pass through the model\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(inputs)\n",
    "    \n",
    "#     # Get the predicted class\n",
    "#     predicted_class_idx = outputs.logits.argmax(-1).item()\n",
    "#     predicted_class_label = model.config.id2label[predicted_class_idx]\n",
    "    \n",
    "#     return predicted_class_label\n",
    "\n",
    "# # Example usage\n",
    "# import cv2\n",
    "# video_path = \"path_to_your_video.mp4\"\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Get the label for the current frame\n",
    "#     label = label_frame(frame)\n",
    "#     print(f\"Frame Label: {label}\")\n",
    "\n",
    "# cap.release()\n",
    "\n",
    "\n",
    "# def label_frames_in_batch(frames):\n",
    "#     # Convert list of frames to list of PIL images\n",
    "#     images = [Image.fromarray(frame) for frame in frames]\n",
    "\n",
    "#     # Preprocess the batch of images\n",
    "#     inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "\n",
    "#     # Forward pass through the model\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "    \n",
    "#     # Get the predicted classes\n",
    "#     predicted_classes = outputs.logits.argmax(-1).tolist()\n",
    "#     labels = [model.config.id2label[idx] for idx in predicted_classes]\n",
    "    \n",
    "#     return labels\n",
    "\n",
    "# # Example usage for batch processing\n",
    "# frames = [...]  # List of frames (images)\n",
    "# labels = label_frames_in_batch(frames)\n",
    "# for frame, label in zip(frames, labels):\n",
    "#     print(f\"Frame Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77924d8b-e93b-416e-b26d-f224ae157069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "# fix for multi-day block - dont get blocked #1 rule -> files were empty -> fix = put files in fiels.hashirama -> look at them \n",
    "#! python -c \"import cv2; print(cv2.getBuildInformation())\" | grep -i ffmpeg\n",
    "#! pip install opencv-contrib-python \n",
    "# semantic seam carving - find arreas of high information entropy  or manual insert keysframs\n",
    "# downlaod video\n",
    "# slice input to frames\n",
    "# use semseg to get objects \n",
    "# img -> svg\n",
    "# squish into a lottie \n",
    "#! pip install pytubefix\n",
    "#https://github.com/pytube/pytube/issues/1894\n",
    "# _default_clients[\"ANDROID\"][\"context\"][\"client\"][\"clientVersion\"] = \"19.08.35\"\n",
    "# _default_clients[\"IOS\"][\"context\"][\"client\"][\"clientVersion\"] = \"19.08.35\"\n",
    "# _default_clients[\"ANDROID_EMBED\"][\"context\"][\"client\"][\"clientVersion\"] = \"19.08.35\"\n",
    "# _default_clients[\"IOS_EMBED\"][\"context\"][\"client\"][\"clientVersion\"] = \"19.08.35\"\n",
    "# _default_clients[\"IOS_MUSIC\"][\"context\"][\"client\"][\"clientVersion\"] = \"6.41\"\n",
    "# _default_clients[\"ANDROID_MUSIC\"] = _default_clients[\"ANDROID_CREATOR\"]\n",
    "# # where to save \n",
    "# can you use similar pipelines for lottie+robot\n",
    "#os.listdir('/home/adnan')\n",
    "#display_img('~/segment-anything-2/output.gif')\n",
    "# link of the video to be downloaded \n",
    "# using a gif - or an svg -> can use calculus to reverse engineer the data\n",
    "# or the function or the scales  - see today and tomororw = slope for ???\n",
    "# instantenous rate of chane\n",
    "# \n",
    "#!micromamba  install yt-dlp\n",
    "#!pip install pytube moviepy\n",
    "#apple = 'https://www.youtube.com/watch?v=cwCMV7LF6S4'\n",
    "#'https://www.youtube.com/watch?v=dQw4w9WgXcQ'\n",
    "#'?si=BvAYyjbT4dkoiM1E'\n",
    "#YouTube URL -> Select object / shapes via segmentanything2 -\n",
    "# > convert filtered video to frames -> for each frame convert to svg \n",
    "#-> svg + lottie transport = good? \n",
    "\n",
    "#deliveraves \n",
    "#1. apple - trust / stripe\n",
    "#2. 10 logos - trust / stripe\n",
    "#3. lgihtbulb - anime\n",
    "#4. flower -  anime\n",
    "#5. kensshin - anime\n",
    "#6. https://youtu.be/WUvTyaaNkzM?si=EXjJZgRb4nO4yOFi - math vis \n",
    "#7. ??? (didactism, \n",
    "# have segment 2 anything working\n",
    "\n",
    "# 1. adding a video\n",
    "\n",
    "# 2. then splitting into frames\n",
    "\n",
    "# 3. Then using segment to extract objects\n",
    "\n",
    "# 4. Then converting the animation to svg\n",
    "\n",
    "# 5. Then animation to lottie. \n",
    "\n",
    "\n",
    "# hopefully should be done in 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d877c5b4-6ec4-4566-833f-153579b19258",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('youtube')\n",
    "#! rm youtube/* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772eaade-64f6-4c25-9dd8-92bce32c1066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"0:06 xubliminal 0:15 orthly  0:19 byter 0:23 reddit 0:30 plane 0:34 ignition 0:38 morrison brewery 0:47 allvit 0:56 upbound 1:03 pigeon post 1:08 bitmoji 1:12 ticket seats 1:18 paperplane games 1:25 sendinblue 1:32 beats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28e6de83-3555-41cd-9d6a-353cd21a3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightly adapted from https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\n",
    "\n",
    "\n",
    "def mask_generate(url=\"../homelab-status-page/static/webcam.jpg\"):\n",
    "    using_colab = False\n",
    "    if using_colab:\n",
    "        import torch\n",
    "        import torchvision\n",
    "        print(\"PyTorch version:\", torch.__version__)\n",
    "        print(\"Torchvision version:\", torchvision.__version__)\n",
    "        print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "        import sys\n",
    "        !{sys.executable} -m pip install opencv-python matplotlib\n",
    "        !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything-2.git'\n",
    "    \n",
    "        !mkdir -p images\n",
    "        !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything-2/main/notebooks/images/cars.jpg\n",
    "    \n",
    "        !mkdir -p ../checkpoints/\n",
    "        !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\n",
    "    \n",
    "    import os\n",
    "    # if using Apple MPS, fall back to CPU for unsupported ops\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    \n",
    "    # select the device for computation\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"using device: {device}\")\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        # use bfloat16 for the entire notebook\n",
    "        torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "        if torch.cuda.get_device_properties(0).major >= 8:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "    elif device.type == \"mps\":\n",
    "        print(\n",
    "            \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "            \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "            \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "        )\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    \n",
    "    def show_anns(anns, borders=True):\n",
    "        if len(anns) == 0:\n",
    "            return\n",
    "        sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "        ax = plt.gca()\n",
    "        ax.set_autoscale_on(False)\n",
    "    \n",
    "        img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "        img[:, :, 3] = 0\n",
    "        for ann in sorted_anns:\n",
    "            m = ann['segmentation']\n",
    "            color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
    "            img[m] = color_mask \n",
    "            if borders:\n",
    "                import cv2\n",
    "                contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "                # Try to smooth contours\n",
    "                contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "                cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1) \n",
    "    \n",
    "        ax.imshow(img)\n",
    "    \n",
    "    image = Image.open(url)\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    from sam2.build_sam import build_sam2\n",
    "    from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "    \n",
    "    sam2_checkpoint = \"./sam2_hiera_tiny.pt\"\n",
    "    model_cfg = \"sam2_hiera_t.yaml\"\n",
    "    \n",
    "    sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)\n",
    "    \n",
    "    mask_generator = SAM2AutomaticMaskGenerator(sam2)\n",
    "    \n",
    "    masks = mask_generator.generate(image)\n",
    "    \n",
    "    print(len(masks))\n",
    "    print(masks[0].keys())\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(image)\n",
    "    show_anns(masks)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    mask_generator_2 = SAM2AutomaticMaskGenerator(\n",
    "        model=sam2,\n",
    "        points_per_side=64,\n",
    "        points_per_batch=128,\n",
    "        pred_iou_thresh=0.7,\n",
    "        stability_score_thresh=0.92,\n",
    "        stability_score_offset=0.7,\n",
    "        crop_n_layers=1,\n",
    "        box_nms_thresh=0.7,\n",
    "        crop_n_points_downscale_factor=2,\n",
    "        min_mask_region_area=25.0,\n",
    "        use_m2m=True,\n",
    "    )\n",
    "    \n",
    "    masks2 = mask_generator_2.generate(image)\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(image)\n",
    "    show_anns(masks2)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7aeea6-8cc7-4c33-8034-999180e22b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JP ART GIF ANMIATION ENGINE OF SCOIECNE\n",
    "\n",
    "\n",
    "def mp4_to_gif(youtube_url: str, start_time: int, duration: int, output_gif_path: str):\n",
    "    try:\n",
    "        with VideoFileClip(video_file_path) as video_clip:\n",
    "            gif_clip = video_clip.subclip(start_time, start_time + duration)\n",
    "            \n",
    "            # Step 3: Write the GIF to a file\n",
    "            gif_clip.write_gif(output_gif_path, fps=15)\n",
    "        \n",
    "        print(f\"GIF saved to {output_gif_path}\")\n",
    "        \n",
    "        # Clean up the downloaded video file\n",
    "        os.remove(video_file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "##youtube_to_gif(\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\", start_time=10, duration=5, output_gif_path=\"output.gif\")\n",
    "\n",
    "# Example usage:\n",
    "youtube_to_gif(test_, start_time=10, duration=5, output_gif_path=\"output.gif\")\n",
    "\n",
    "\n",
    "\n",
    "# a = [1,2,3]\n",
    "# b  = [4,5,6]\n",
    "\n",
    "# c = a + b\n",
    "\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5160a-5617-45f8-96a2-ca415a33076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import replicate\n",
    " # yolo od\n",
    "# ! pip install replicate\n",
    "\n",
    "# print(output)\n",
    "output = replicate.run(\n",
    "    \"zsxkib/yolo-world:d232445620610b78671a7f288f37bf3baec831537503e9064afcf0bfd0f0a151\",\n",
    "    input={\n",
    "        \"nms_thr\": 0.5,\n",
    "        \"score_thr\": 0.05,\n",
    "        \"class_names\": \"dog, eye, tongue, ear, leash, backpack, person, nose\",\n",
    "        \"input_media\": \"https://replicate.delivery/pbxt/KOJpWfZmaP6tUv8fqR2n0z3FdBhtytoP5llaecrvvez0p4LE/dog.jpeg\",\n",
    "        \"return_json\": False,\n",
    "        \"max_num_boxes\": 100\n",
    "    }\n",
    ")\n",
    "print(output)\n",
    " #import replicate\n",
    " # \n",
    "# output = replicate.run(\n",
    "#   \"stability-ai/sdxl:39ed52f2a78e934b3ba6e2a89f5b1c712de7dfea535525255b1aa35c5565e08b\",\n",
    "#   input={\"prompt\": \"an iguana on the beach, pointillism\"}\n",
    "# )\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54005d6d-5f89-4eeb-9f63-b5715a411acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find animations - open parameters to make them scrollable / interactive\n",
    "# https://www.viggle.ai/home\n",
    "# https://replicate.com/explore\n",
    "# https://replicate.com/datacte/proteus-v0.3?prediction=3dfwjqrsxnrgg0chxtxbxvj9qm\n",
    "# https://replicate.com/collections/image-to-text\n",
    "# https://replicate.com/collections/image-editing\n",
    "# https://replicate.com/collections/text-recognition-ocr\n",
    "# https://replicate.com/collections/text-to-video\n",
    "# https://replicate.com/black-forest-labs/flux-dev\n",
    "# https://replicate.com/black-forest-labs/flux-schnell\n",
    "# https://huggingface.co/TencentARC/InstantMesh\n",
    "# https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1\n",
    "# https://huggingface.co/finegrain/finegrain-box-segmenter\n",
    "# https://huggingface.co/microsoft/xclip-large-patch14-16-frames\n",
    "# https://huggingface.co/Vchitect/SEINE\n",
    "# https://huggingface.co/yisol/IDM-VTON\n",
    "# https://huggingface.co/jameslahm/yolov10n\n",
    "# https://huggingface.co/fancyfeast/joytag\n",
    "# https://huggingface.co/depth-anything/Depth-Anything-V2-Large\n",
    "\n",
    "# !uv pip install gspread pandas oauth2client\n",
    "# #https://platform.openai.com/docs/quickstart\n",
    "# ! uv pip install openai\n",
    "\n",
    "#reblob games + distl\n",
    "# unreal / brunon simons\n",
    "# neuron \n",
    "#find the ocntrols the govern the systems behavior and tewaekr till it works\n",
    "\n",
    "# ##https://github.com/barbagroup/AeroPython\n",
    "\n",
    "# # jeremey howardb\n",
    "# # peter norvig\n",
    "# # lucumr\n",
    "# # simonw\n",
    "# # https://github.com/thombashi/sqlitebiter\n",
    "# # https://github.com/topics/python\n",
    "# from IPython.display import HTML, display\n",
    "\n",
    "# html_content = \"\"\"\n",
    "# <div style=\"border: 2px solid black; padding: 10px;\">\n",
    "#     <h1>Hello, this is an embedded HTML block!</h1>\n",
    "#     <p>You can style this with CSS and include various HTML elements.</p>\n",
    "# </div>\n",
    "# \"\"\"\n",
    "\n",
    "# display(HTML(html_content))\n",
    "# os.listdir('.')\n",
    "\n",
    "\n",
    "# from IPython.display import HTML, display\n",
    "\n",
    "# html_content = \"\"\"\n",
    "# <div style=\"border: 2px solid black; padding: 10px;\">\n",
    "#     <h1>Hello, this is an embedded HTML block!</h1>\n",
    "#     <img src=\"./output.gif\">You can style this with CSS and include various HTML elements.</p>\n",
    "# </div>\n",
    "# \"\"\"\n",
    "\n",
    "# display(HTML(html_content))\n",
    "\n",
    "\n",
    "anime_100 = [\n",
    "    \"Sousou no Frieren\",\n",
    "    \"Fullmetal Alchemist: Brotherhood\",\n",
    "    \"Steins;Gate\",\n",
    "    \"Gintama°\",\n",
    "    \"Shingeki no Kyojin Season 3 Part 2\",\n",
    "    \"Gintama: The Final\",\n",
    "    \"Gintama'\",\n",
    "    \"Hunter x Hunter (2011)\",\n",
    "    \"Monogatari Series: Off & Monster Season\",\n",
    "    \"Ginga Eiyuu Densetsu\",\n",
    "    \"Gintama': Enchousen\",\n",
    "    \"Bleach: Sennen Kessen-hen\",\n",
    "    \"Kaguya-sama wa Kokurasetai: Ultra Romantic\",\n",
    "    \"Gintama.\",\n",
    "    \"Fruits Basket: The Final\",\n",
    "    \"Clannad: After Story\",\n",
    "    \"Gintama\",\n",
    "    \"Koe no Katachi\",\n",
    "    \"3-gatsu no Lion 2nd Season\",\n",
    "    \"Code Geass: Hangyaku no Lelouch R2\",\n",
    "    \"Gintama Movie 2: Kanketsu-hen - Yorozuya yo Eien Nare\",\n",
    "    \"Kusuriya no Hitorigoto\",\n",
    "    \"Monster\",\n",
    "    \"Gintama.: Shirogane no Tamashii-hen - Kouhan-sen\",\n",
    "    \"Shingeki no Kyojin: The Final Season - Kanketsu-hen\",\n",
    "    \"Owarimonogatari 2nd Season\",\n",
    "    \"Violet Evergarden Movie\",\n",
    "    \"Kimi no Na wa.\",\n",
    "    \"Kingdom 3rd Season\",\n",
    "    \"Vinland Saga Season 2\",\n",
    "    \"Gintama.: Shirogane no Tamashii-hen\",\n",
    "    \"Boku no Kokoro no Yabai Yatsu 2nd Season\",\n",
    "    \"Jujutsu Kaisen 2nd Season\",\n",
    "    \"Mob Psycho 100 II\",\n",
    "    \"Kizumonogatari III: Reiketsu-hen\",\n",
    "    \"Shingeki no Kyojin: The Final Season\",\n",
    "    \"Bocchi the Rock!\",\n",
    "    \"Haikyuu!! Karasuno Koukou vs. Shiratorizawa Gakuen Koukou\",\n",
    "    \"Hajime no Ippo\",\n",
    "    \"Sen to Chihiro no Kamikakushi\",\n",
    "    \"Kaguya-sama wa Kokurasetai: First Kiss wa Owaranai\",\n",
    "    \"Monogatari Series: Second Season\",\n",
    "    \"Shingeki no Kyojin: The Final Season Part 2\",\n",
    "    \"Vinland Saga\",\n",
    "    \"The First Slam Dunk\",\n",
    "    \"Cowboy Bebop\",\n",
    "    \"Hibike! Euphonium 3\",\n",
    "    \"Kimetsu no Yaiba: Yuukaku-hen\",\n",
    "    \"Kingdom 4th Season\",\n",
    "    \"Kingdom 5th Season\"\n",
    "] +[\n",
    "    \"Shiguang Dailiren\",\n",
    "    \"Ashita no Joe 2\",\n",
    "    \"Mushishi Zoku Shou 2nd Season\",\n",
    "    \"One Piece\",\n",
    "    \"Look Back\",\n",
    "    \"Mob Psycho 100 III\",\n",
    "    \"Shouwa Genroku Rakugo Shinjuu: Sukeroku Futatabi-hen\",\n",
    "    \"86 Part 2\",\n",
    "    \"Bleach: Sennen Kessen-hen - Ketsubetsu-tan\",\n",
    "    \"Code Geass: Hangyaku no Lelouch\",\n",
    "    \"Rurouni Kenshin: Meiji Kenkaku Romantan - Tsuioku-hen\",\n",
    "    \"Mushishi Zoku Shou\",\n",
    "    \"Great Teacher Onizuka\",\n",
    "    \"Mo Dao Zu Shi: Wanjie Pian\",\n",
    "    \"Violet Evergarden\",\n",
    "    \"Tian Guan Cifu Er\",\n",
    "    \"Hajime no Ippo: New Challenger\",\n",
    "    \"Howl no Ugoku Shiro\",\n",
    "    \"Haikyuu!! Movie: Gomisuteba no Kessen\",\n",
    "    \"Mononoke Hime\",\n",
    "    \"Odd Taxi\",\n",
    "    \"Mushishi\",\n",
    "    \"Mushoku Tensei: Isekai Ittara Honki Dasu Part 2\",\n",
    "    \"Fate/stay night Movie: Heaven's Feel - III. Spring Song\",\n",
    "    \"Bungou Stray Dogs 5th Season\",\n",
    "    \"Shigatsu wa Kimi no Uso\",\n",
    "    \"Made in Abyss\",\n",
    "    \"Natsume Yuujinchou Shi\",\n",
    "    \"Kaguya-sama wa Kokurasetai? Tensai-tachi no Renai Zunousen\",\n",
    "    \"Shiguang Dailiren II\",\n",
    "    \"Shingeki no Kyojin Season 3\",\n",
    "    \"Tengen Toppa Gurren Lagann\",\n",
    "    \"\\\"Oshi no Ko\\\"\",\n",
    "    \"Death Note\",\n",
    "    \"Haikyuu!! Second Season\",\n",
    "    \"Made in Abyss: Retsujitsu no Ougonkyou\",\n",
    "    \"Ping Pong the Animation\",\n",
    "    \"Made in Abyss Movie 3: Fukaki Tamashii no Reimei\",\n",
    "    \"Natsume Yuujinchou Roku\",\n",
    "    \"Dungeon Meshi\",\n",
    "    \"Cyberpunk: Edgerunners\",\n",
    "    \"Hajime no Ippo: Rising\",\n",
    "    \"Suzumiya Haruhi no Shoushitsu\",\n",
    "    \"Kimi ni Todoke 3rd Season\",\n",
    "    \"Kenpuu Denki Berserk\",\n",
    "    \"Mushishi Zoku Shou: Suzu no Shizuku\",\n",
    "    \"Seishun Buta Yarou wa Yumemiru Shoujo no Yume wo Minai\",\n",
    "    \"Shin Evangelion Movie:||\",\n",
    "    \"Tengen Toppa Gurren Lagann Movie 2: Lagann-hen\",\n",
    "    \"JoJo no Kimyou na Bouken Part 5: Ougon no Kaze\"\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
