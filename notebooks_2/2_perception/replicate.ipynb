{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd18525-6371-4fa7-9216-a47b50282309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "def image_predictor(url=\"../homelab-status-page/static/webcam.jpg\"):\n",
    "    \n",
    "    using_colab = False\n",
    "    if using_colab:\n",
    "        import torch\n",
    "        import torchvision\n",
    "        print(\"PyTorch version:\", torch.__version__)\n",
    "        print(\"Torchvision version:\", torchvision.__version__)\n",
    "        print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "        import sys\n",
    "        !{sys.executable} -m pip install opencv-python matplotlib\n",
    "        !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything-2.git'\n",
    "    \n",
    "        !mkdir -p images\n",
    "        !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything-2/main/notebooks/images/truck.jpg\n",
    "        !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything-2/main/notebooks/images/groceries.jpg\n",
    "    \n",
    "        !mkdir -p ../checkpoints/\n",
    "        !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\n",
    "    import os\n",
    "    # if using Apple MPS, fall back to CPU for unsupported ops\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    # select the device for computation\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"using device: {device}\")\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        # use bfloat16 for the entire notebook\n",
    "        torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "        if torch.cuda.get_device_properties(0).major >= 8:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "    elif device.type == \"mps\":\n",
    "        print(\n",
    "            \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "            \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "            \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "        )\n",
    "    np.random.seed(3)\n",
    "    \n",
    "    def show_mask(mask, ax, random_color=False, borders = True):\n",
    "        if random_color:\n",
    "            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "        else:\n",
    "            color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "        h, w = mask.shape[-2:]\n",
    "        mask = mask.astype(np.uint8)\n",
    "        mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "        if borders:\n",
    "            import cv2\n",
    "            contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "            # Try to smooth contours\n",
    "            contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "            mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n",
    "        ax.imshow(mask_image)\n",
    "    \n",
    "    def show_points(coords, labels, ax, marker_size=375):\n",
    "        pos_points = coords[labels==1]\n",
    "        neg_points = coords[labels==0]\n",
    "        ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "        ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "    def show_box(box, ax):\n",
    "        x0, y0 = box[0], box[1]\n",
    "        w, h = box[2] - box[0], box[3] - box[1]\n",
    "        ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n",
    "    \n",
    "    def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "        for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(image)\n",
    "            show_mask(mask, plt.gca(), borders=borders)\n",
    "            if point_coords is not None:\n",
    "                assert input_labels is not None\n",
    "                show_points(point_coords, input_labels, plt.gca())\n",
    "            if box_coords is not None:\n",
    "                # boxes\n",
    "                show_box(box_coords, plt.gca())\n",
    "            if len(scores) > 1:\n",
    "                plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    image = Image.open(url)\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('on')\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    from sam2.build_sam import build_sam2\n",
    "    from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "    \n",
    "    sam2_checkpoint = \"sam2_hiera_tiny.pt\"\n",
    "    model_cfg = \"sam2_hiera_t.yaml\"\n",
    "    \n",
    "    sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
    "    \n",
    "    predictor = SAM2ImagePredictor(sam2_model)\n",
    "    predictor.set_image(image)\n",
    "    input_point = np.array([[500, 375]])\n",
    "    input_label = np.array([1])\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    show_points(input_point, input_label, plt.gca())\n",
    "    plt.axis('on')\n",
    "    plt.show()  \n",
    "    print(predictor._features[\"image_embed\"].shape, predictor._features[\"image_embed\"][-1].shape)\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=True,\n",
    "    )\n",
    "    sorted_ind = np.argsort(scores)[::-1]\n",
    "    masks = masks[sorted_ind]\n",
    "    scores = scores[sorted_ind]\n",
    "    logits = logits[sorted_ind]\n",
    "    masks.shape  # (number_of_masks) x H x W\n",
    "    show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label, borders=True)\n",
    "    input_point = np.array([[500, 375], [1125, 625]])\n",
    "    input_label = np.array([1, 1])\n",
    "    \n",
    "    mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n",
    "    masks, scores, _ = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        mask_input=mask_input[None, :, :],\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    masks.shape\n",
    "    show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)\n",
    "    input_point = np.array([[500, 375], [1125, 625]])\n",
    "    input_label = np.array([1, 0])\n",
    "    \n",
    "    mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n",
    "    masks, scores, _ = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        mask_input=mask_input[None, :, :],\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)\n",
    "    input_box = np.array([425, 600, 700, 875])\n",
    "    masks, scores, _ = predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=input_box[None, :],\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    show_masks(image, masks, scores, box_coords=input_box)\n",
    "    input_box = np.array([425, 600, 700, 875])\n",
    "    input_point = np.array([[575, 750]])\n",
    "    input_label = np.array([0])\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        box=input_box,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    show_masks(image, masks, scores, box_coords=input_box, point_coords=input_point, input_labels=input_label)\n",
    "    input_boxes = np.array([\n",
    "        [75, 275, 1725, 850],\n",
    "        [425, 600, 700, 875],\n",
    "        [1375, 550, 1650, 800],\n",
    "        [1240, 675, 1400, 750],\n",
    "    ])\n",
    "    masks, scores, _ = predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=input_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    masks.shape  # (batch_size) x (num_predicted_masks_per_input) x H x W\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    for mask in masks:\n",
    "        show_mask(mask.squeeze(0), plt.gca(), random_color=True)\n",
    "    for box in input_boxes:\n",
    "        show_box(box, plt.gca())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    image1 = image  # truck.jpg from above\n",
    "    image1_boxes = np.array([\n",
    "        [75, 275, 1725, 850],\n",
    "        [425, 600, 700, 875],\n",
    "        [1375, 550, 1650, 800],\n",
    "        [1240, 675, 1400, 750],\n",
    "    ])\n",
    "    \n",
    "    image2 = Image.open(url)\n",
    "    image2 = np.array(image2.convert(\"RGB\"))\n",
    "    image2_boxes = np.array([\n",
    "        [450, 170, 520, 350],\n",
    "        [350, 190, 450, 350],\n",
    "        [500, 170, 580, 350],\n",
    "        [580, 170, 640, 350],\n",
    "    ])\n",
    "    \n",
    "    img_batch = [image1, image2]\n",
    "    boxes_batch = [image1_boxes, image2_boxes]\n",
    "    predictor.set_image_batch(img_batch)\n",
    "    masks_batch, scores_batch, _ = predictor.predict_batch(\n",
    "        None,\n",
    "        None, \n",
    "        box_batch=boxes_batch, \n",
    "        multimask_output=False\n",
    "    )\n",
    "    for image, boxes, masks in zip(img_batch, boxes_batch, masks_batch):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)   \n",
    "        for mask in masks:\n",
    "            show_mask(mask.squeeze(0), plt.gca(), random_color=True)\n",
    "        for box in boxes:\n",
    "            show_box(box, plt.gca())\n",
    "    image1 = image  # truck.jpg from above\n",
    "    image1_pts = np.array([\n",
    "        [[500, 375]],\n",
    "        [[650, 750]]\n",
    "        ]) # Bx1x2 where B corresponds to number of objects \n",
    "    image1_labels = np.array([[1], [1]])\n",
    "    \n",
    "    image2_pts = np.array([\n",
    "        [[400, 300]],\n",
    "        [[630, 300]],\n",
    "    ])\n",
    "    image2_labels = np.array([[1], [1]])\n",
    "    \n",
    "    pts_batch = [image1_pts, image2_pts]\n",
    "    labels_batch = [image1_labels, image2_labels]\n",
    "    masks_batch, scores_batch, _ = predictor.predict_batch(pts_batch, labels_batch, box_batch=None, multimask_output=True)\n",
    "    \n",
    "    # Select the best single mask per object\n",
    "    best_masks = []\n",
    "    for masks, scores in zip(masks_batch,scores_batch):\n",
    "        best_masks.append(masks[range(len(masks)), np.argmax(scores, axis=-1)])\n",
    "    for image, points, labels, masks in zip(img_batch, pts_batch, labels_batch, best_masks):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)   \n",
    "        for mask in masks:\n",
    "            show_mask(mask, plt.gca(), random_color=True)\n",
    "        show_points(points, labels, plt.gca())\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17683bbe-4bd5-49b0-85b8-b87250d46292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e0be73-8539-4929-b399-d1aad26bd655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24720fb1-93a0-4fec-828d-32db94efb4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb60a2ae-94fb-47fe-b262-b35e88b2c361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f109a65-1835-46f0-9e0c-6c787dc78d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4fd1e6-57b2-4a91-956d-5fb462ffc29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed34982-a1a4-4911-99d6-1ce100d5d59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a715f10-ea00-4903-bc4c-ba8395c2eaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac20512e-b13c-4d6e-833a-beb825d9c374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146c3d7-92bf-42d9-a28a-3982b0f9ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from replicate.client import Client\n",
    "\n",
    "\n",
    "replicate = Client(\n",
    "#  api_token=os.environ[\"REPLICATE_API_TOKEN\"],\n",
    "    api_token=api_key,\n",
    "  # headers={\n",
    "  #   \"User-Agent\": \"my-app/1.0\"\n",
    "  # }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c7ef5-2a3c-4039-b964-ff81db7de983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import replicate\n",
    "#https://blog.jupyter.org/generative-ai-in-jupyter-3f7174824862\n",
    "# yolo - extents\n",
    "# sobel filter - webgl\n",
    "# semseg\n",
    "# once that works -> check in this repo\n",
    "\n",
    "current_webcam_image = \"https://hashirama.blog/static/webcam.jpg\"\n",
    "\n",
    "#vision tranformer\n",
    "output = replicate.run(\n",
    "    \"adirik/grounding-dino:efd10a8ddc57ea28773327e881ce95e20cc1d734c589f7dd01d2036921ed78aa\",\n",
    "    input={\n",
    "        \"image\": current_webcam_image,\n",
    "        \"query\": \"pink mug\",\n",
    "        \"box_threshold\": 0.2,\n",
    "        \"text_threshold\": 0.2,\n",
    "        \"show_visualisation\": True\n",
    "    }\n",
    ")\n",
    "print(output)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b68e9f-0f92-4b6a-9137-22d1006e49f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.viggle.ai/home\n",
    "# https://replicate.com/explore\n",
    "# https://replicate.com/datacte/proteus-v0.3?prediction=3dfwjqrsxnrgg0chxtxbxvj9qm\n",
    "# https://replicate.com/collections/image-to-text\n",
    "# https://replicate.com/collections/image-editing\n",
    "# https://replicate.com/collections/text-recognition-ocr\n",
    "# https://replicate.com/collections/text-to-video\n",
    "# https://replicate.com/black-forest-labs/flux-dev\n",
    "# https://replicate.com/black-forest-labs/flux-schnell\n",
    "# https://huggingface.co/TencentARC/InstantMesh\n",
    "# https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1\n",
    "# https://huggingface.co/finegrain/finegrain-box-segmenter\n",
    "# https://huggingface.co/microsoft/xclip-large-patch14-16-frames\n",
    "# https://huggingface.co/Vchitect/SEINE\n",
    "# https://huggingface.co/yisol/IDM-VTON\n",
    "# https://huggingface.co/jameslahm/yolov10n\n",
    "# https://huggingface.co/fancyfeast/joytag\n",
    "# https://huggingface.co/depth-anything/Depth-Anything-V2-Large\n",
    "\n",
    "# !uv pip install gspread pandas oauth2client\n",
    "# #https://platform.openai.com/docs/quickstart\n",
    "# ! uv pip install openai\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `frame_data` contains your binary frame data (like from an HTTP response or WebSocket)\n",
    "\n",
    "# Convert the binary data into an image\n",
    "#img = Image.open(BytesIO(frame_data))\n",
    "\n",
    "input_image_path = os.path.expanduser(\"~/hashirama/services/homelab-status-page/static/webcam.jpg\")\n",
    "output_image_path = os.path.expanduser(\"~/hashirama/services/homelab-status-page/static/jupyter_webcam.jpg\")\n",
    "import os\n",
    "from io import BytesIO\n",
    "def display_img(path):\n",
    "    frame_data = open(path, \"rb\").read()\n",
    "    img = Image.open(BytesIO(frame_data))\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "display_img(input_image_path)\n",
    "\n",
    "def overwrite_img(input_image_path):\n",
    "    with Image.open(input_image_path) as img:\n",
    "        # Flip the image (horizontally or vertically)\n",
    "        flipped_img = ImageOps.mirror(img)  # Horizontal flip\n",
    "        # Invert the colors of the image\n",
    "        inverted_img = ImageOps.invert(flipped_img.convert(\"RGB\"))\n",
    "        \n",
    "        # Save the modified image\n",
    "        inverted_img.save(output_image_path)\n",
    "\n",
    "overwrite_img((input_image_path))\n",
    "\n",
    "display_img(output_image_path)\n",
    "#reblob games + distl\n",
    "# unreal / brunon simons\n",
    "# neuron \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18af46e-1575-4008-8115-e59f81b0e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/facebookresearch/segment-anything-2/blob/main/notebooks/image_predictor_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46454865-5f8a-4356-8fb4-a2e7d121021e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda975d2-1b18-416d-b480-4aca0730a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightly adapted from https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\n",
    "\n",
    "\n",
    "def mask_generate(url=\"../homelab-status-page/static/webcam.jpg\"):\n",
    "    using_colab = False\n",
    "    if using_colab:\n",
    "        import torch\n",
    "        import torchvision\n",
    "        print(\"PyTorch version:\", torch.__version__)\n",
    "        print(\"Torchvision version:\", torchvision.__version__)\n",
    "        print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "        import sys\n",
    "        !{sys.executable} -m pip install opencv-python matplotlib\n",
    "        !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything-2.git'\n",
    "    \n",
    "        !mkdir -p images\n",
    "        !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything-2/main/notebooks/images/cars.jpg\n",
    "    \n",
    "        !mkdir -p ../checkpoints/\n",
    "        !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\n",
    "    \n",
    "    import os\n",
    "    # if using Apple MPS, fall back to CPU for unsupported ops\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    \n",
    "    # select the device for computation\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"using device: {device}\")\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        # use bfloat16 for the entire notebook\n",
    "        torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "        if torch.cuda.get_device_properties(0).major >= 8:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "    elif device.type == \"mps\":\n",
    "        print(\n",
    "            \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "            \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "            \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "        )\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    \n",
    "    def show_anns(anns, borders=True):\n",
    "        if len(anns) == 0:\n",
    "            return\n",
    "        sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "        ax = plt.gca()\n",
    "        ax.set_autoscale_on(False)\n",
    "    \n",
    "        img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "        img[:, :, 3] = 0\n",
    "        for ann in sorted_anns:\n",
    "            m = ann['segmentation']\n",
    "            color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
    "            img[m] = color_mask \n",
    "            if borders:\n",
    "                import cv2\n",
    "                contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "                # Try to smooth contours\n",
    "                contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "                cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1) \n",
    "    \n",
    "        ax.imshow(img)\n",
    "    \n",
    "    image = Image.open(url)\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    from sam2.build_sam import build_sam2\n",
    "    from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "    \n",
    "    sam2_checkpoint = \"./sam2_hiera_tiny.pt\"\n",
    "    model_cfg = \"sam2_hiera_t.yaml\"\n",
    "    \n",
    "    sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)\n",
    "    \n",
    "    mask_generator = SAM2AutomaticMaskGenerator(sam2)\n",
    "    \n",
    "    masks = mask_generator.generate(image)\n",
    "    \n",
    "    print(len(masks))\n",
    "    print(masks[0].keys())\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(image)\n",
    "    show_anns(masks)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    mask_generator_2 = SAM2AutomaticMaskGenerator(\n",
    "        model=sam2,\n",
    "        points_per_side=64,\n",
    "        points_per_batch=128,\n",
    "        pred_iou_thresh=0.7,\n",
    "        stability_score_thresh=0.92,\n",
    "        stability_score_offset=0.7,\n",
    "        crop_n_layers=1,\n",
    "        box_nms_thresh=0.7,\n",
    "        crop_n_points_downscale_factor=2,\n",
    "        min_mask_region_area=25.0,\n",
    "        use_m2m=True,\n",
    "    )\n",
    "    \n",
    "    masks2 = mask_generator_2.generate(image)\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(image)\n",
    "    show_anns(masks2)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415dc4e-42d1-4e00-8d23-12bb9851b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "#https://githubb.com/enjalot/latent-scope/blob/main/notebooks/any-transformer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a70d050-313f-430e-b7b2-29a34b64f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "# horizontal / vertical notebook - diffrent views on same data - annotaiton\n",
    "# Define paths\n",
    "# Expand '~' to the full user path\n",
    "# input_image_path = os.path.expanduser(input_image_path)\n",
    "# output_image_path = os.path.expanduser(output_image_path)\n",
    "\n",
    "input_image_path = os.path.expanduser(\"~/hashirama/services/homelab-status-page/static/webcam.jpg\")\n",
    "output_image_path = os.path.expanduser(\"~/hashirama/services/homelab-status-page/static/jupyter_webcam.jpg\")\n",
    "#input_image_path = 'https://hashirama.blog/static/webcam.jpg'\n",
    "#https://hashirama.blog/static/jupyter_webcam.jpg\";\n",
    "\n",
    "# Open the image\n",
    "\n",
    "def overwrite_img():\n",
    "    with Image.open(input_image_path) as img:\n",
    "        # Flip the image (horizontally or vertically)\n",
    "        flipped_img = ImageOps.mirror(img)  # Horizontal flip\n",
    "        # Invert the colors of the image\n",
    "        inverted_img = ImageOps.invert(flipped_img.convert(\"RGB\"))\n",
    "        \n",
    "        # Save the modified image\n",
    "        inverted_img.save(output_image_path)\n",
    "\n",
    "    print(f\"Processed image saved to {output_image_path}\")\n",
    "overwrite_img()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
