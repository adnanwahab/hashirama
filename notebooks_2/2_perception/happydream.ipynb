{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf99563-8f71-4969-b663-cf615e5774c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1235b-9acb-4483-b67d-8ca3188f2413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - [] - cognition engine - happydream (jeff heer) -> 1 lottie - ligthbulb -> 10:30-12\n",
    "# - [] - Observable.next - 1:30\n",
    "# - [] - flirtflow - dont project - agentic ui - 3\n",
    "\n",
    "\n",
    "\n",
    "# curate what knowledge - \n",
    "\n",
    "# ai makes robotis tractable - and easy\n",
    "# robotics makes 70% of current human endeavors easy and fun \n",
    "\n",
    "\n",
    "# https://robertheaton.com/archive/\n",
    "# https://www.kalzumeus.com/archive/\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "#\n",
    "# \n",
    "\n",
    "# make a playwright end_point(list) -> get shit \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1 blog = 5000 tokens\n",
    "# llm -> convert 1 billion tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119f251-dd03-49fa-9c20-0876c5c568c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfminerimport cv2\n",
    "import numpy as np\n",
    "import cv2\n",
    "# Load the image\n",
    "#Seeing Spaces\n",
    "image = cv2.imread('../homelab-status-page/static/images/blog/Seeing Spaces.jpg')\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply edge detection\n",
    "edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "\n",
    "# Find contours (lines) in the image\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Sort contours by area (largest to smallest)\n",
    "contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "# Loop through the detected contours and crop them\n",
    "for i, contour in enumerate(contours):\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    cropped_image = image[y:y+h, x:x+w]\n",
    "    cv2.imwrite(f'cropped_image_{i}.png', cropped_image)\n",
    "\n",
    "# Show the final result\n",
    "cv2.imshow('Cropped Images', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#_ = ['http://www.vpri.org/work/ifnct.html http://wwimport cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the image\n",
    "# image = cv2.imread('/mnt/data/image.png')\n",
    "\n",
    "# # Convert to grayscale\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # Apply edge detection\n",
    "# edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "\n",
    "# # Find contours (lines) in the image\n",
    "# contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# # Sort contours by area (largest to smallest)\n",
    "# contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "# # Loop through the detected contours and crop them\n",
    "# for i, contour in enumerate(contours):\n",
    "#     x, y, w, h = cv2.boundingRect(contour)\n",
    "#     cropped_image = image[y:y+h, x:x+w]\n",
    "#     cv2.imwrite(f'cropped_image_{i}.png', cropped_image)\n",
    "\n",
    "# # Show the final result\n",
    "# cv2.imshow('Cropped Images', image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "# w.vpri.org/work/tlpiese.html',\n",
    "# 'http://www.vpri.org/words_links/links_tlpiese.htm',\n",
    "# 'http://www.vpri.org/work/pichri.html', 'http://www.vpri.org/work/uitald_olpc.html',\n",
    "# 'http://www.vpri.org/work/ifnct.html',\n",
    "# 'https://tinlizzie.org/IA/index.php/Talks_by_Alan_Kay',\n",
    "# 'https://tinlizzie.org/IA/index.php/Papers_from_Viewpoints_Research_Institute',\n",
    "# 'https://d1wqtxts1xzle7.cloudfront.net/39850910/Planning_and_Visualization_for_Automated20151109-22967-16vh9pi-libre.pdf?1447128058=&response-content-disposition=inline%3B+filename%3DPlanning_and_Visualization_for_Automated.pdf&Expires=1727548730&Signature=Q364HZ9o6ERQZBDRn4gOW3A4gfN~YjOVUbxyofurTVewpWnnWJlFqBtUa4Qejw1d5BVp2zATZBCBQAf8eShpPs9iRKlTvnpoHwdmWD9~Ml7fNH9G-ZJX5Y05IK~pKIczhkL9rFWPS4bqIcX-opoCkAyUEr8GBX3o6YeT~tyZFsNVxg50P6IXVurJbDIRTMCsJ4OUHpVp~-syc1tdYDJHPJ4JNWo-VDD1UvCauWhq5NTTEaT8JmFtp5dnsbMnDMiMvq6r77Ax4~bSzg9pgjNivzJVDssCTot4sgb4UiqB6mlCDvjQRWLyp1LHafpgKIpfuSMwVq7tgTgdgAUjXqpW0Q__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA',\n",
    "# 'https://www.youtube.com/watch?v=dP-NureH_-w',\n",
    "#     ]\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# #from pdfminer.high_level import extract_text\n",
    "# #from pptx import Presentation\n",
    "\n",
    "# def download_html(url, save_dir):\n",
    "#     response = requests.get(url)\n",
    "#     response.raise_for_status()\n",
    "#     filename = os.path.join(save_dir, f\"{url.split('//')[-1].replace('/', '_')}.html\")\n",
    "#     with open(filename, 'w', encoding='utf-8') as file:\n",
    "#         file.write(response.text)\n",
    "\n",
    "# def download_pdf(url, save_dir):\n",
    "#     response = requests.get(url)\n",
    "#     response.raise_for_status()\n",
    "#     filename = os.path.join(save_dir, f\"{url.split('//')[-1].replace('/', '_')}.pdf\")\n",
    "#     with open(filename, 'wb') as file:\n",
    "#         file.write(response.content)\n",
    "\n",
    "# def download_pptx(url, save_dir):\n",
    "#     response = requests.get(url)\n",
    "#     response.raise_for_status()\n",
    "#     filename = os.path.join(save_dir, f\"{url.split('//')[-1].replace('/', '_')}.pptx\")\n",
    "#     with open(filename, 'wb') as file:\n",
    "#         file.write(response.content)\n",
    "\n",
    "# def download_youtube_video(url, save_dir):\n",
    "#     # Placeholder for YouTube video download logic using `pytube` or `youtube_dl`\n",
    "#     pass\n",
    "\n",
    "# def save_assets(urls, save_dir):\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "    \n",
    "#     for url in urls:\n",
    "#         if url.endswith(\".pdf\"):\n",
    "#             download_pdf(url, save_dir)\n",
    "#         elif url.endswith(\".ppt\") or url.endswith(\".pptx\"):\n",
    "#             download_pptx(url, save_dir)\n",
    "#         elif \"youtube.com\" in url:\n",
    "#             download_youtube_video(url, save_dir)\n",
    "#         else:\n",
    "#             download_html(url, save_dir)\n",
    "#         print(f\"Downloaded: {url}\")\n",
    "\n",
    "# # List of URLs to download\n",
    "# urls = [\n",
    "#     \"http://www.vpri.org/work/ifnct.html\",\n",
    "#     \"http://www.vpri.org/work/tlpiese.html\",\n",
    "#     \"http://www.vpri.org/words_links/links_tlpiese.htm\",\n",
    "#     \"http://www.vpri.org/work/pichri.html\",\n",
    "#     \"http://www.vpri.org/work/uitald_olpc.html\",\n",
    "#     \"http://www.vpri.org/work/ifnct.html\",\n",
    "#     \"https://tinlizzie.org/IA/index.php/Talks_by_Alan_Kay\",\n",
    "#     \"https://tinlizzie.org/IA/index.php/Papers_from_Viewpoints_Research_Institute\",\n",
    "# #    \"https://d1wqtxts1xzle7.cloudfront.net/39850910/Planning_and_Visualization_for_Automated20151109-22967-16vh9pi-libre.pdf\",\n",
    "#     \"https://www.youtube.com/watch?v=dP-NureH_-w\"\n",
    "# ]\n",
    "\n",
    "# # Specify the directory where you want to save the files\n",
    "# save_directory = \"../homelab-status-page/static/alan_kay/\"\n",
    "\n",
    "# # Call the function to save assets\n",
    "# save_assets(urls, save_directory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84808b-0c2e-4d1a-a25e-8c932917ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [] \n",
    "#whos e line is it anyway\n",
    "# replace your tthoguhts with alan kay\n",
    "# search + gather all reseources on kay, bret victor \n",
    "# get all slides, references, core_ideases\n",
    "\n",
    "\n",
    "\n",
    "secret_gospel = 'https://d1wqtxts1xzle7.cloudfront.net/39850910/Planning_and_Visualization_for_Automated20151109-22967-16vh9pi-libre.pdf'\n",
    "def prompt_to_animation(prompt):\n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "def pdf_to_animation(pdf_url):\n",
    "    return \n",
    "\n",
    "    blogs = [\n",
    "    'https://www.latent.space/',\n",
    "    'https://ai.meta.com/sam2/',\n",
    "    'https://www.latent.space/',\n",
    "    'https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg',\n",
    "    'https://www.youtube.com/computerphile',\n",
    "    'https://link.springer.com/article/10.1007/s40747-021-00428-4',\n",
    "    'https://macwright.com/',\n",
    "    'https://richzhang.github.io/colorization/',\n",
    "    'https://macwright.com/2024/07/30/reverse-engineer-a-day',\n",
    "    'https://apenwarr.ca/log/?m=202304,\n",
    "]\n",
    "#get jp better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d07bfe3a-522f-4f2f-b44b-36834c8fb31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zoox = \"../homelab-status-page/static/data/zoox_papers.json\"\n",
    "waymo = \"../homelab-status-page/static/data/waymo_papers.json\"\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def get_papers():\n",
    "    return read_papers(zoox) + read_papers(waymo)\n",
    "\n",
    "def read_papers(zoox):\n",
    "    has_papers = []\n",
    "    with open(zoox) as f:\n",
    "        d = json.load(f)\n",
    "        #print(d)\n",
    "        for person in d:\n",
    "            #print(person)\n",
    "            has_papers += d[person].split('\\n')\n",
    "    return has_papers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32b83356-67f9-4af7-99b4-90866acb94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cool = get_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9b1dff1-193d-4ece-9ba0-70933ea0bc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_SelfD_Self-Learning_Large-Scale_Driving_Policies_From_the_Web_CVPR_2022_paper.pdf',\n",
       " 'https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Visual_Point_Cloud_Forecasting_enables_Scalable_Autonomous_Driving_CVPR_2024_paper.pdf',\n",
       " 'https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Neural_Kernel_Surface_Reconstruction_CVPR_2023_paper.pdf',\n",
       " 'http://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Neural_LiDAR_Fields_for_Novel_View_Synthesis_ICCV_2023_paper.pdf',\n",
       " 'http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_SurfelGAN_Synthesizing_Realistic_Sensor_Data_for_Autonomous_Driving_CVPR_2020_paper.pdf',\n",
       " 'https://openaccess.thecvf.com/content/CVPR2024/papers/Eskandar_An_Empirical_Study_of_the_Generalization_Ability_of_Lidar_3D_CVPR_2024_paper.pdf',\n",
       " 'http://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Learning_Human_Dynamics_in_Autonomous_Driving_Scenarios_ICCV_2023_paper.pdf',\n",
       " 'https://sites.gatech.edu/ece-tusharkishna/files/2023/02/TusharKrishna_CV.pdf',\n",
       " 'https://www.usenix.org/system/files/hotedge20_paper_kolosov.pdf',\n",
       " 'https://link.springer.com/content/pdf/10.1007/978-3-030-75977-3.pdf',\n",
       " 'https://www.usenix.org/system/files/usenixsecurity23-xiao-qifan.pdf',\n",
       " 'https://openaccess.thecvf.com/content/ICCV2021/papers/Luo_Exploring_Simple_3D_Multi-Object_Tracking_for_Autonomous_Driving_ICCV_2021_paper.pdf',\n",
       " 'http://openaccess.thecvf.com/content_CVPR_2020/papers/Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.pdf',\n",
       " 'https://mountainscholar.org/bitstream/10217/235264/1/Trabelsi_colostate_0053A_17016.pdf',\n",
       " 'https://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Nguyen_Multi-Camera_Multiple_3D_Object_Tracking_on_the_Move_for_Autonomous_CVPRW_2022_paper.pdf',\n",
       " 'https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Beyond_3D_Siamese_Tracking_A_Motion-Centric_Paradigm_for_3D_Single_CVPR_2022_paper.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel7/6287639/6514899/10242101.pdf',\n",
       " 'http://openaccess.thecvf.com/content/CVPR2023/papers/Liu_FlatFormer_Flattened_Window_Attention_for_Efficient_Point_Cloud_Transformer_CVPR_2023_paper.pdf',\n",
       " 'https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Towards_Large-scale_3D_Representation_Learning_with_Multi-dataset_Point_Prompt_Training_CVPR_2024_paper.pdf',\n",
       " 'https://irom-lab.princeton.edu/wp-content/uploads/2023/08/Robotics_snapshot.pdf',\n",
       " 'http://old-eclass.uop.gr/modules/document/file.php/DIT209/%CF%80%CF%81%CE%BF%CF%84%CE%B5%CE%B9%CE%BD%CF%8C%CE%BC%CE%B5%CE%BD%CE%B7%20%CE%B8%CE%B5%CE%BC%CE%B1%CF%84%CE%BF%CE%BB%CE%BF%CE%B3%CE%AF%CE%B1%202019-2020/Security%20Modeling%20of%20Autonomous%20Systems%2C%20A%20Survey.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel7/10203037/10203050/10205229.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel7/9878378/9878366/09879045.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel8/10654794/10654797/10656301.pdf',\n",
       " 'https://www.bilbloggen.dk/wp-content/uploads/2023/04/Autonomous-Vehicle-Implementation-Predictions.pdf',\n",
       " 'https://www.frontierglobalpartners.com/wp-content/uploads/2019/09/FGP-Opportunities-in-the-5G-Buildout-Sep2019-2-Compressed_04.pdf',\n",
       " 'https://proceedings.neurips.cc/paper_files/paper/2020/file/1a669e81c8093745261889539694be7f-Paper.pdf',\n",
       " 'http://essay.utwente.nl/98115/1/beretta_BA_psychology.pdf',\n",
       " 'https://proceedings.neurips.cc/paper_files/paper/2023/file/1838feeb71c4b4ea524d0df2f7074245-Paper-Datasets_and_Benchmarks.pdf',\n",
       " 'https://proceedings.neurips.cc/paper_files/paper/2023/file/b96ce67b2f2d45e4ab315e13a6b5b9c5-Paper-Datasets_and_Benchmarks.pdf',\n",
       " 'https://openaccess.thecvf.com/content/CVPR2023/papers/Suo_MixSim_A_Hierarchical_Framework_for_Mixed_Reality_Traffic_Simulation_CVPR_2023_paper.pdf',\n",
       " 'https://openaccess.thecvf.com/content/ICCV2023/papers/Najibi_Unsupervised_3D_Perception_with_2D_Vision-Language_Distillation_for_Autonomous_Driving_ICCV_2023_paper.pdf',\n",
       " 'https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/papers/Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.pdf',\n",
       " 'https://openaccess.thecvf.com/content/CVPR2023/papers/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.pdf',\n",
       " 'http://openaccess.thecvf.com/content/CVPR2021/papers/Wang_AdvSim_Generating_Safety-Critical_Scenarios_for_Self-Driving_Vehicles_CVPR_2021_paper.pdf',\n",
       " 'https://research-assets.waabi.ai/SceneControl/paper.pdf',\n",
       " 'https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/papers/Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.pdf',\n",
       " 'https://opus4.kobv.de/opus4-haw/files/862/I000797910Thesis.pdf',\n",
       " 'https://mediatum.ub.tum.de/doc/1395259/document.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel8/10654794/10654797/10656301.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel8/10654794/10654797/10656301.pdf',\n",
       " 'https://web2py.iiit.ac.in/research_centres/publications/download/mastersthesis.pdf.84308e536bd7f929.416e757261675f536168755f5f323031383132313030345f5f4d61737465725f5468657369732d332e706466.pdf',\n",
       " 'https://openaccess.thecvf.com/content/CVPR2021/papers/Qian_Robust_Multimodal_Vehicle_Detection_in_Foggy_Weather_Using_Complementary_Lidar_CVPR_2021_paper.pdf',\n",
       " 'http://www.weisongshi.org/papers/gao22-ADSecurity.pdf',\n",
       " 'http://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Learning_Human_Dynamics_in_Autonomous_Driving_Scenarios_ICCV_2023_paper.pdf',\n",
       " 'https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_PACER_On-Demand_Pedestrian_Animation_Controller_in_Driving_Scenarios_CVPR_2024_paper.pdf',\n",
       " 'https://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.pdf',\n",
       " 'http://openaccess.thecvf.com/content/ICCV2021/papers/Ettinger_Large_Scale_Interactive_Motion_Forecasting_for_Autonomous_Driving_The_Waymo_ICCV_2021_paper.pdf',\n",
       " 'https://yuanxzhang.github.io/paper/sctrans-icse24.pdf',\n",
       " 'https://www.researchgate.net/profile/Cheng_Chang52/publication/365115956_MetaScenario_A_Framework_for_Driving_Scenario_Data_Description_Storage_and_Indexing/links/63a7edb4c3c99660eba1fa6a/MetaScenario-A-Framework-for-Driving-Scenario-Data-Description-Storage-and-Indexing.pdf',\n",
       " 'https://napier-repository.worktribe.com/preview/3643801/Overtaking%20Mechanisms%20Based%20on%20Augmented%20Intelligence%20for%20Autonomous%20Driving%20Datasets_%20Methods_%20and%20Challenges_compressed.pdf',\n",
       " 'https://proceedings.neurips.cc/paper_files/paper/2023/file/1838feeb71c4b4ea524d0df2f7074245-Paper-Datasets_and_Benchmarks.pdf',\n",
       " 'http://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf',\n",
       " 'https://proceedings.neurips.cc/paper_files/paper/2023/file/b96ce67b2f2d45e4ab315e13a6b5b9c5-Paper-Datasets_and_Benchmarks.pdf',\n",
       " 'https://openaccess.thecvf.com/content/CVPR2023/papers/Suo_MixSim_A_Hierarchical_Framework_for_Mixed_Reality_Traffic_Simulation_CVPR_2023_paper.pdf',\n",
       " 'https://link.springer.com/content/pdf/10.1007/978-981-97-1749-1.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel7/9577055/9577056/09578286.pdf',\n",
       " 'https://core.ac.uk/download/pdf/429673697.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel7/9856930/9856648/09857463.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel7/9878378/9878366/09879045.pdf',\n",
       " 'https://openaccess.thecvf.com/content_CVPR_2020/papers/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.pdf',\n",
       " 'https://openaccess.thecvf.com/content/ICCV2021/papers/Luo_Exploring_Simple_3D_Multi-Object_Tracking_for_Autonomous_Driving_ICCV_2021_paper.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel8/34/4359286/10614862.pdf',\n",
       " 'http://openaccess.thecvf.com/content/ICCV2021/papers/Ettinger_Large_Scale_Interactive_Motion_Forecasting_for_Autonomous_Driving_The_Waymo_ICCV_2021_paper.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel7/9577055/9577056/09578286.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel8/10654794/10654797/10656301.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel7/9878378/9878366/09879045.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel7/10203037/10203050/10205229.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel7/10376473/10376477/10376960.pdf',\n",
       " 'https://ieeexplore.ieee.org/iel7/9706406/9706408/09706967.pdf']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "cool = [paper for paper in is_cool if len(paper) > 0]\n",
    "coolb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af09cd4d-2a34-42c0-b901-c9977cf8aa14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Assuming the PDF content is embedded within an HTML tag,\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# we'll extract it using BeautifulSoup\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m pdf_content \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpdf-content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Save the extracted PDF content to a temporary file\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fpdf import FPDF\n",
    "\n",
    "urls = cool\n",
    "# Create a PDF document\n",
    "pdf = FPDF()\n",
    "\n",
    "# Iterate over the list of URLs and download them as PDF files\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Assuming the PDF content is embedded within an HTML tag,\n",
    "    # we'll extract it using BeautifulSoup\n",
    "    pdf_content = soup.find('object', id='pdf-content').get('data')\n",
    "    \n",
    "    # Save the extracted PDF content to a temporary file\n",
    "    with open(\"temp.pdf\", \"wb\") as f:\n",
    "        f.write(pdf_content)\n",
    "        \n",
    "    # Convert the temporary PDF file to a bytes object using FPDF\n",
    "    pdf_data = open(\"temp.pdf\", \"rb\").read()\n",
    "    \n",
    "    # Add the downloaded PDF to the main document\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=15)\n",
    "    pdf.cell(0, 10, txt=\"Downloaded from: \" + url, ln=True, align='L')\n",
    "    pdf.image(bytesio.BytesIO(pdf_data), w=200, h=150)\n",
    "\n",
    "# Save the final PDF document\n",
    "pdf.output(\"downloaded_links.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d2cf32a-3fa9-4f71-b3d5-93e0a53ece87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! mkdir pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6ca1527-3650-4c4b-a9a7-7ce5f169379d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zhang_SelfD_Self-Learning_Large-Scale_Driving_Policies_From_the_Web_CVPR_2022_paper.pdf\n",
      "Zhang_SelfD_Self-Learning_Large-Scale_Driving_Policies_From_the_Web_CVPR_2022_paper.pdf\n",
      "PDF saved successfully as ./pdf/Zhang_SelfD_Self-Learning_Large-Scale_Driving_Policies_From_the_Web_CVPR_2022_paper.pdf\n",
      "Yang_Visual_Point_Cloud_Forecasting_enables_Scalable_Autonomous_Driving_CVPR_2024_paper.pdf\n",
      "Yang_Visual_Point_Cloud_Forecasting_enables_Scalable_Autonomous_Driving_CVPR_2024_paper.pdf\n",
      "PDF saved successfully as ./pdf/Yang_Visual_Point_Cloud_Forecasting_enables_Scalable_Autonomous_Driving_CVPR_2024_paper.pdf\n",
      "Huang_Neural_Kernel_Surface_Reconstruction_CVPR_2023_paper.pdf\n",
      "Huang_Neural_Kernel_Surface_Reconstruction_CVPR_2023_paper.pdf\n",
      "PDF saved successfully as ./pdf/Huang_Neural_Kernel_Surface_Reconstruction_CVPR_2023_paper.pdf\n",
      "Huang_Neural_LiDAR_Fields_for_Novel_View_Synthesis_ICCV_2023_paper.pdf\n",
      "Huang_Neural_LiDAR_Fields_for_Novel_View_Synthesis_ICCV_2023_paper.pdf\n",
      "PDF saved successfully as ./pdf/Huang_Neural_LiDAR_Fields_for_Novel_View_Synthesis_ICCV_2023_paper.pdf\n",
      "Yang_SurfelGAN_Synthesizing_Realistic_Sensor_Data_for_Autonomous_Driving_CVPR_2020_paper.pdf\n",
      "Yang_SurfelGAN_Synthesizing_Realistic_Sensor_Data_for_Autonomous_Driving_CVPR_2020_paper.pdf\n",
      "PDF saved successfully as ./pdf/Yang_SurfelGAN_Synthesizing_Realistic_Sensor_Data_for_Autonomous_Driving_CVPR_2020_paper.pdf\n",
      "Eskandar_An_Empirical_Study_of_the_Generalization_Ability_of_Lidar_3D_CVPR_2024_paper.pdf\n",
      "Eskandar_An_Empirical_Study_of_the_Generalization_Ability_of_Lidar_3D_CVPR_2024_paper.pdf\n",
      "PDF saved successfully as ./pdf/Eskandar_An_Empirical_Study_of_the_Generalization_Ability_of_Lidar_3D_CVPR_2024_paper.pdf\n",
      "Wang_Learning_Human_Dynamics_in_Autonomous_Driving_Scenarios_ICCV_2023_paper.pdf\n",
      "Wang_Learning_Human_Dynamics_in_Autonomous_Driving_Scenarios_ICCV_2023_paper.pdf\n",
      "PDF saved successfully as ./pdf/Wang_Learning_Human_Dynamics_in_Autonomous_Driving_Scenarios_ICCV_2023_paper.pdf\n",
      "TusharKrishna_CV.pdf\n",
      "TusharKrishna_CV.pdf\n",
      "PDF saved successfully as ./pdf/TusharKrishna_CV.pdf\n",
      "hotedge20_paper_kolosov.pdf\n",
      "hotedge20_paper_kolosov.pdf\n",
      "PDF saved successfully as ./pdf/hotedge20_paper_kolosov.pdf\n",
      "978-3-030-75977-3.pdf\n",
      "978-3-030-75977-3.pdf\n",
      "PDF saved successfully as ./pdf/978-3-030-75977-3.pdf\n",
      "usenixsecurity23-xiao-qifan.pdf\n",
      "usenixsecurity23-xiao-qifan.pdf\n",
      "PDF saved successfully as ./pdf/usenixsecurity23-xiao-qifan.pdf\n",
      "Luo_Exploring_Simple_3D_Multi-Object_Tracking_for_Autonomous_Driving_ICCV_2021_paper.pdf\n",
      "Luo_Exploring_Simple_3D_Multi-Object_Tracking_for_Autonomous_Driving_ICCV_2021_paper.pdf\n",
      "PDF saved successfully as ./pdf/Luo_Exploring_Simple_3D_Multi-Object_Tracking_for_Autonomous_Driving_ICCV_2021_paper.pdf\n",
      "Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.pdf\n",
      "Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.pdf\n",
      "PDF saved successfully as ./pdf/Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.pdf\n",
      "Trabelsi_colostate_0053A_17016.pdf\n",
      "Trabelsi_colostate_0053A_17016.pdf\n",
      "PDF saved successfully as ./pdf/Trabelsi_colostate_0053A_17016.pdf\n",
      "Nguyen_Multi-Camera_Multiple_3D_Object_Tracking_on_the_Move_for_Autonomous_CVPRW_2022_paper.pdf\n",
      "Nguyen_Multi-Camera_Multiple_3D_Object_Tracking_on_the_Move_for_Autonomous_CVPRW_2022_paper.pdf\n",
      "PDF saved successfully as ./pdf/Nguyen_Multi-Camera_Multiple_3D_Object_Tracking_on_the_Move_for_Autonomous_CVPRW_2022_paper.pdf\n",
      "Zheng_Beyond_3D_Siamese_Tracking_A_Motion-Centric_Paradigm_for_3D_Single_CVPR_2022_paper.pdf\n",
      "Zheng_Beyond_3D_Siamese_Tracking_A_Motion-Centric_Paradigm_for_3D_Single_CVPR_2022_paper.pdf\n",
      "PDF saved successfully as ./pdf/Zheng_Beyond_3D_Siamese_Tracking_A_Motion-Centric_Paradigm_for_3D_Single_CVPR_2022_paper.pdf\n",
      "Failed to download PDF. Status code: 418\n",
      "Liu_FlatFormer_Flattened_Window_Attention_for_Efficient_Point_Cloud_Transformer_CVPR_2023_paper.pdf\n",
      "Liu_FlatFormer_Flattened_Window_Attention_for_Efficient_Point_Cloud_Transformer_CVPR_2023_paper.pdf\n",
      "PDF saved successfully as ./pdf/Liu_FlatFormer_Flattened_Window_Attention_for_Efficient_Point_Cloud_Transformer_CVPR_2023_paper.pdf\n",
      "Wu_Towards_Large-scale_3D_Representation_Learning_with_Multi-dataset_Point_Prompt_Training_CVPR_2024_paper.pdf\n",
      "Wu_Towards_Large-scale_3D_Representation_Learning_with_Multi-dataset_Point_Prompt_Training_CVPR_2024_paper.pdf\n",
      "PDF saved successfully as ./pdf/Wu_Towards_Large-scale_3D_Representation_Learning_with_Multi-dataset_Point_Prompt_Training_CVPR_2024_paper.pdf\n",
      "Robotics_snapshot.pdf\n",
      "Robotics_snapshot.pdf\n",
      "PDF saved successfully as ./pdf/Robotics_snapshot.pdf\n",
      "Security%20Modeling%20of%20Autonomous%20Systems%2C%20A%20Survey.pdf\n",
      "Security%20Modeling%20of%20Autonomous%20Systems%2C%20A%20Survey.pdf\n",
      "PDF saved successfully as ./pdf/Security%20Modeling%20of%20Autonomous%20Systems%2C%20A%20Survey.pdf\n",
      "Failed to download PDF. Status code: 418\n",
      "Failed to download PDF. Status code: 418\n",
      "Failed to download PDF. Status code: 418\n",
      "Autonomous-Vehicle-Implementation-Predictions.pdf\n",
      "Autonomous-Vehicle-Implementation-Predictions.pdf\n",
      "PDF saved successfully as ./pdf/Autonomous-Vehicle-Implementation-Predictions.pdf\n",
      "FGP-Opportunities-in-the-5G-Buildout-Sep2019-2-Compressed_04.pdf\n",
      "FGP-Opportunities-in-the-5G-Buildout-Sep2019-2-Compressed_04.pdf\n",
      "PDF saved successfully as ./pdf/FGP-Opportunities-in-the-5G-Buildout-Sep2019-2-Compressed_04.pdf\n",
      "1a669e81c8093745261889539694be7f-Paper.pdf\n",
      "1a669e81c8093745261889539694be7f-Paper.pdf\n",
      "PDF saved successfully as ./pdf/1a669e81c8093745261889539694be7f-Paper.pdf\n",
      "beretta_BA_psychology.pdf\n",
      "beretta_BA_psychology.pdf\n",
      "PDF saved successfully as ./pdf/beretta_BA_psychology.pdf\n",
      "1838feeb71c4b4ea524d0df2f7074245-Paper-Datasets_and_Benchmarks.pdf\n",
      "1838feeb71c4b4ea524d0df2f7074245-Paper-Datasets_and_Benchmarks.pdf\n",
      "PDF saved successfully as ./pdf/1838feeb71c4b4ea524d0df2f7074245-Paper-Datasets_and_Benchmarks.pdf\n",
      "b96ce67b2f2d45e4ab315e13a6b5b9c5-Paper-Datasets_and_Benchmarks.pdf\n",
      "b96ce67b2f2d45e4ab315e13a6b5b9c5-Paper-Datasets_and_Benchmarks.pdf\n",
      "PDF saved successfully as ./pdf/b96ce67b2f2d45e4ab315e13a6b5b9c5-Paper-Datasets_and_Benchmarks.pdf\n",
      "Suo_MixSim_A_Hierarchical_Framework_for_Mixed_Reality_Traffic_Simulation_CVPR_2023_paper.pdf\n",
      "Suo_MixSim_A_Hierarchical_Framework_for_Mixed_Reality_Traffic_Simulation_CVPR_2023_paper.pdf\n",
      "PDF saved successfully as ./pdf/Suo_MixSim_A_Hierarchical_Framework_for_Mixed_Reality_Traffic_Simulation_CVPR_2023_paper.pdf\n",
      "Najibi_Unsupervised_3D_Perception_with_2D_Vision-Language_Distillation_for_Autonomous_Driving_ICCV_2023_paper.pdf\n",
      "Najibi_Unsupervised_3D_Perception_with_2D_Vision-Language_Distillation_for_Autonomous_Driving_ICCV_2023_paper.pdf\n",
      "PDF saved successfully as ./pdf/Najibi_Unsupervised_3D_Perception_with_2D_Vision-Language_Distillation_for_Autonomous_Driving_ICCV_2023_paper.pdf\n",
      "Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.pdf\n",
      "Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.pdf\n",
      "PDF saved successfully as ./pdf/Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.pdf\n",
      "Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.pdf\n",
      "Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.pdf\n",
      "PDF saved successfully as ./pdf/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.pdf\n",
      "Wang_AdvSim_Generating_Safety-Critical_Scenarios_for_Self-Driving_Vehicles_CVPR_2021_paper.pdf\n",
      "Wang_AdvSim_Generating_Safety-Critical_Scenarios_for_Self-Driving_Vehicles_CVPR_2021_paper.pdf\n",
      "PDF saved successfully as ./pdf/Wang_AdvSim_Generating_Safety-Critical_Scenarios_for_Self-Driving_Vehicles_CVPR_2021_paper.pdf\n",
      "paper.pdf\n",
      "paper.pdf\n",
      "PDF saved successfully as ./pdf/paper.pdf\n",
      "Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.pdf\n",
      "Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.pdf\n",
      "PDF saved successfully as ./pdf/Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.pdf\n",
      "I000797910Thesis.pdf\n",
      "I000797910Thesis.pdf\n",
      "PDF saved successfully as ./pdf/I000797910Thesis.pdf\n",
      "document.pdf\n",
      "document.pdf\n",
      "PDF saved successfully as ./pdf/document.pdf\n",
      "Failed to download PDF. Status code: 418\n",
      "Failed to download PDF. Status code: 418\n"
     ]
    },
    {
     "ename": "SSLError",
     "evalue": "HTTPSConnectionPool(host='cdn.iiit.ac.in', port=443): Max retries exceeded with url: /cdn/web2py.iiit.ac.in/research_centres/publications/download/mastersthesis.pdf.84308e536bd7f929.416e757261675f536168755f5f323031383132313030345f5f4d61737465725f5468657369732d332e706466.pdf (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/urllib3/connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/urllib3/connectionpool.py:404\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/urllib3/connectionpool.py:1061\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1061\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/urllib3/connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# for the host.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/urllib3/util/ssl_.py:458\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 458\u001b[0m     ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/urllib3/util/ssl_.py:502\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    510\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/ssl.py:1375\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1375\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/urllib3/connectionpool.py:802\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    800\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 802\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/urllib3/util/retry.py:594\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    596\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='cdn.iiit.ac.in', port=443): Max retries exceeded with url: /cdn/web2py.iiit.ac.in/research_centres/publications/download/mastersthesis.pdf.84308e536bd7f929.416e757261675f536168755f5f323031383132313030345f5f4d61737465725f5468657369732d332e706466.pdf (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdf_url \u001b[38;5;129;01min\u001b[39;00m cool: \n\u001b[0;32m---> 24\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Check if the request was successful\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;66;03m# Step 3: Save the PDF to a file\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/requests/sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/requests/adapters.py:698\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mSSLError\u001b[0m: HTTPSConnectionPool(host='cdn.iiit.ac.in', port=443): Max retries exceeded with url: /cdn/web2py.iiit.ac.in/research_centres/publications/download/mastersthesis.pdf.84308e536bd7f929.416e757261675f536168755f5f323031383132313030345f5f4d61737465725f5468657369732d332e706466.pdf (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# Step 1: Read the JSON file\n",
    "# with open('pdf_links.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "#     pdf_url = data['pdf_url']\n",
    "\n",
    "# Step 2: Download the PDF\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_name(url) :\n",
    "    # Using os.path.basename()\n",
    "    filename = os.path.basename(urlparse(url).path)\n",
    "    print(filename)  # Output: file.pdf\n",
    "    \n",
    "    # Alternatively, using the 'split' method\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    print(filename)\n",
    "    return filename\n",
    "\n",
    "for pdf_url in cool: \n",
    "    response = requests.get(pdf_url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Step 3: Save the PDF to a file\n",
    "        pdf_filename = \"./pdf/\"+ get_name(pdf_url)\n",
    "        with open(pdf_filename, 'wb') as pdf_file:\n",
    "            pdf_file.write(response.content)\n",
    "        print(f\"PDF saved successfully as {pdf_filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download PDF. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b29f19-74b0-4f31-97c5-5eea379777cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ./pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7636bd7e-c0cd-4c60-80e2-328260b5ab1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'example.txt' exists.\n",
      "Last modified: 2024-09-18 20:34:51\n",
      "Time difference in seconds: 0.10291862487792969\n",
      "The file was modified less than one week ago.\n"
     ]
    }
   ],
   "source": [
    "# batch quests - lottie+gif = first step to this \n",
    "#! touch example.txt\n",
    "def get_sources():\n",
    "    import os\n",
    "    pass \n",
    "import os\n",
    "import time\n",
    "\n",
    "def updates_sources():\n",
    "    return 10\n",
    "\n",
    "# Function to check if a file exists and when it was last modified\n",
    "def check_file_status(file_path):\n",
    "    one_week = 7 * 24 * 60 * 60\n",
    "    if os.path.exists(file_path):\n",
    "        # Get current time\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Get last modified time\n",
    "        last_modified_time = os.path.getmtime(file_path)\n",
    "        \n",
    "        # Calculate the time difference in seconds\n",
    "        time_difference = current_time - last_modified_time\n",
    "        \n",
    "        # One week in seconds (7 days * 24 hours * 60 minutes * 60 seconds)\n",
    "        one_week_in_seconds = 7 * 24 * 60 * 60\n",
    "        \n",
    "        # Convert last modified time to human-readable format\n",
    "        last_modified_time_human = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(last_modified_time))\n",
    "        \n",
    "        print(f\"The file '{file_path}' exists.\")\n",
    "        print(f\"Last modified: {last_modified_time_human}\")\n",
    "        print(f\"Time difference in seconds: {time_difference}\")\n",
    "        \n",
    "        # Check if the file was modified within the last week\n",
    "        if time_difference < one_week_in_seconds:\n",
    "            print(\"The file was modified less than one week ago.\")\n",
    "        else:\n",
    "            cool = updates_sources()\n",
    "            # replace old db and version it\n",
    "            print(\"The file was modified more than one week ago.\")\n",
    "\n",
    "# Example usage\n",
    "file_path = 'example.txt'\n",
    "check_file_status(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a50ccb-c6b4-4d28-b367-8c4285c6d598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en-US\"><head><meta charset=\"UTF-8\">\n",
      "<title>Dynamicland front shelf</title>\n",
      "<link rel=\"canonical\" href=\"/\">\n",
      "<link rel=\"me\" href=\"https://posts.dynamic.land/@dynamicland\">\n",
      "<meta name=\"description\" content=\"Incubating a humane dynamic medium.\">\n",
      "<meta name=\"twitter:card\" content=\"summary_large_image\">\n",
      "<meta name=\"twitter:site\" content=\"@Dynamicland1\">\n",
      "<meta name=\"twitter:creator\" content=\"@Dynamicland1\">\n",
      "<meta name=\"twitter:title\" content=\"Dynamicland\">\n",
      "<meta name=\"twitter:description\" content=\"Incubating a humane dynamic medium.\">\n",
      "<meta name=\"twitter:image\" content=\"https://dynamicland.org/2024/Front_shelf/8c7c199d7fe30d0b695537dcb6d7a702.jpg\">\n",
      "<meta property=\"og:url\" content=\"https://dynamicland.org/\">\n",
      "<meta property=\"og:title\" content=\"Dynamicland\">\n",
      "<meta property=\"og:type\" content=\"website\">\n",
      "<meta property=\"og:description\" content=\"Incubating a humane dynamic medium.\">\n",
      "<meta property=\"og:image\" content=\"https://dynamicland.org/2024/Front_shelf/8c7c199d7fe30d0b695537dcb6d7a702.jpg\">\n",
      "\n",
      "<style>\n",
      "    * { margin:0; padding:0; }\n",
      "    body { background:black; }\n",
      "    .imagemap     { position:relative; margin-left:auto; margin-right:auto; max-width:1400px; margin-bottom:10px; }\n",
      "    .imagemap:last-child { margin-bottom:0; }\n",
      "    .imagemap img { display:block; width:100%; height:auto; }\n",
      "\n",
      "    body.horizontal { height:100vh; }\n",
      "    .horizontal .imagemap { width:fit-content; height:100%; max-width:10000px; margin-bottom:0; }\n",
      "    .horizontal .imagemap img { width:auto; height:100%; }\n",
      "\n",
      "    .imagemap a   { display:block; position:absolute; }\n",
      "    .imagemap a:hover   { background: rgba(0,0,0,0.20); }\n",
      "    .reveal .imagemap a { background: rgba(0,0,0,0.20); }\n",
      "    .imagemap a:hover   { box-shadow: 0 0 8px rgba(0,0,0,0.50); }\n",
      "    .reveal .imagemap a { box-shadow: 0 0 8px rgba(0,0,0,0.50); }\n",
      "    .header { font: normal 12px/1.0 Helvetica, Arial, sans-serif; color:#888; text-align:center; padding:10px; }\n",
      "    .imagemap a.fade-hover:hover { box-shadow:none; background:linear-gradient(180deg,rgba(0,0,0,0.0),25%,rgba(0,0,0,0.3)); }\n",
      "</style>\n",
      "<script type=\"text/javascript\">\n",
      "let modifiers = { \"16\":1, \"17\":1, \"91\":1, \"93\":1 }  // shift, control, left command, right command\n",
      "window.addEventListener(\"keydown\", function (e) { if (modifiers[e.keyCode+\"\"]) { document.body.classList.add(\"reveal\"); }});\n",
      "window.addEventListener(\"keyup\", function (e) { if (modifiers[e.keyCode+\"\"]) { document.body.classList.remove(\"reveal\"); }});\n",
      "window.addEventListener(\"blur\", function (e) { document.body.classList.remove(\"reveal\"); });\n",
      "</script>\n",
      "\n",
      "</head><body class=\"vertical\">\n",
      "\n",
      "<div id=\"p1\" class=\"imagemap\"><img src=\"/2024/Front_shelf/2dc5b9c5984d24df5d2aeaedf06442f8.jpg\" width=\"879\" height=\"1600\" alt=\"6-minute intro, 2024\n",
      "Video, 6 min\n",
      "Dynamicland is a prototype of a humane dynamic medium. Here is a quick overview.\n",
      "\n",
      "Dynamicland, 2018\n",
      "Presentation, 45 min\n",
      "Dynamicland is a computer that is a place, where people work together in the real world. This talk introduces the Dynamicland spirit.\n",
      "\n",
      "Bootstrapping Research, 2019\n",
      "Presentation, 30 min\n",
      "Dynamicland is a bootstrapping research lab. The lab itself is the experiment. This talk introduces this uniquely effective approach.\n",
      "\n",
      "Nonprofit narrative, 2020\n",
      "Essay, 5 pages\n",
      "Dynamicland is nonprofit. Here is why this technology cannot be a product, but must be a form of education and community practice.\n",
      "\n",
      "Progress report 2014-2019\n",
      "Photo-comic, 8 pages\n",
      "Concise history of the labâs first five years, and the origins of Realtalk and Dynamicland.\n",
      "\n",
      "Progress report 2020\n",
      "Video, 20 min\n",
      "Informal tour of the lab, shortly after the start of the pandemic, showing progress on the new Realtalk.\n",
      "\n",
      "Progress report 2021\n",
      "Handwritten letter, 8 pages\n",
      "Informal letter to the community, describing progress through the pandemic and future plans.\n",
      "\n",
      "Progress report 2022\n",
      "Photo-comic, 10 pages\n",
      "Progress in 2022, centering around work in the science lab.\n",
      "\n",
      "The Humane Representation of Thought, 2014\n",
      "Poster, 10 ft. Presentation, 55 min\n",
      "Founding document for the lab. The long-term vision of a spatial dynamic medium.\n",
      "\n",
      "Dynamicland zine, 2017\n",
      "Zine, 16 pages\n",
      "Founding document for the Dynamicland community space, depicting the vision, principles, and progress.\n",
      "\n",
      "The Library, 2019\n",
      "Fiction, 2 pages\n",
      "One component of the long-term vision for the dynamic medium.\n",
      "\n",
      "The communal science lab, 2024\n",
      "Booklet, 24 pages\n",
      "Founding document for the bionanotechnology lab, depicting the vision of communal science.\n",
      "\n",
      "Publications\n",
      "Selected articles, reports, and presentations.\n",
      "\n",
      "Archive\n",
      "All projects, big and small.\n",
      "\n",
      "People\n",
      "Who did this.\n",
      "\n",
      "FAQ\n",
      "Questions and answers.\"><a href=\"/2024/Intro/\" style=\"left:11.45%;top:18.27%;width:17.64%;height:5.55%;\" title=\"6-minute intro, 2024\" >\n",
      "</a><a href=\"/2018/Dynamicland/\" style=\"left:32.57%;top:18.52%;width:17.59%;height:5.25%;\" title=\"Dynamicland, 2018\" >\n",
      "</a><a href=\"/2019/Bootstrapping_Research/\" style=\"left:52.09%;top:18.39%;width:17.55%;height:5.45%;\" title=\"Bootstrapping Research, 2019\" >\n",
      "</a><a href=\"/2020/Nonprofit_narrative.pdf\" style=\"left:73.23%;top:18.50%;width:17.73%;height:5.42%;\" title=\"Nonprofit narrative, 2020\" >\n",
      "</a><a href=\"/2019/Progress_report/\" style=\"left:9.25%;top:35.71%;width:17.59%;height:5.70%;\" title=\"Progress report 2014-2019\" >\n",
      "</a><a href=\"/2020/Progress_report/\" style=\"left:30.34%;top:35.81%;width:17.59%;height:5.60%;\" title=\"Progress report 2020\" >\n",
      "</a><a href=\"/2021/Progress_report/\" style=\"left:50.45%;top:35.83%;width:17.55%;height:5.60%;\" title=\"Progress report 2021\" >\n",
      "</a><a href=\"/2022/Progress_report/\" style=\"left:71.84%;top:35.92%;width:17.77%;height:5.52%;\" title=\"Progress report 2022\" >\n",
      "</a><a href=\"/2014/CDG_research_agendas/\" style=\"left:8.43%;top:53.20%;width:17.23%;height:5.75%;\" title=\"The Humane Representation of Thought, 2014\" >\n",
      "</a><a href=\"/2017/Zine/\" style=\"left:29.95%;top:53.21%;width:17.36%;height:5.92%;\" title=\"Dynamicland zine, 2017\" >\n",
      "</a><a href=\"/2019/The_Library.pdf\" style=\"left:51.95%;top:53.27%;width:17.55%;height:5.90%;\" title=\"The Library, 2019\" >\n",
      "</a><a href=\"/2024/The_communal_science_lab.pdf\" style=\"left:73.73%;top:53.31%;width:17.82%;height:5.82%;\" title=\"The communal science lab, 2024\" >\n",
      "</a><a href=\"/publications/\" style=\"left:7.87%;top:70.66%;width:17.25%;height:5.90%;\" title=\"Publications\" >\n",
      "</a><a href=\"/archive/\" style=\"left:31.52%;top:70.68%;width:17.59%;height:6.02%;\" title=\"Archive\" >\n",
      "</a><a href=\"/2023/People/\" style=\"left:53.07%;top:70.81%;width:17.14%;height:5.95%;\" title=\"People\" >\n",
      "</a><a href=\"/2024/FAQ/\" style=\"left:75.45%;top:70.81%;width:17.64%;height:6.05%;\" title=\"FAQ\" >\n",
      "</a><a href=\"/2024/Roots/\" style=\"left:0.00%;top:78.00%;width:100.00%;height:22.00%;\" title=\"\" class='fade-hover'>\n",
      "</a></div>\n",
      "</body></html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Replace with your desired API endpoint\n",
    "url = \"https://dynamicland.org/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    data = response.text  # or response.text for raw content\n",
    "    print(data)\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb9c53c9-eff2-4884-876f-8fa8cc81a382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adnan/hashirama/services/perception\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "!pwd\n",
    "! cat ../homelab-status-page/static/images/blog/botparty.jpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3535c75d-2fd1-440f-8a71-34e872487519",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file '../homelab-status-page/static/images/blog/botparty.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Open the image\u001b[39;00m\n\u001b[1;32m      4\u001b[0m input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../homelab-status-page/static/images/blog/botparty.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convert to RGB if necessary\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/PIL/Image.py:3283\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3281\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m   3282\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[0;32m-> 3283\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '../homelab-status-page/static/images/blog/botparty.jpeg'"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageOps\n",
    "\n",
    "# Open the image\n",
    "input_path = \"../homelab-status-page/static/images/blog/botparty.jpeg\"\n",
    "img = Image.open(input_path)\n",
    "\n",
    "# Convert to RGB if necessary\n",
    "if img.mode != 'RGB':\n",
    "    img = img.convert('RGB')\n",
    "\n",
    "# Invert the colors\n",
    "inverted_img = ImageOps.invert(img)\n",
    "\n",
    "# Convert the white background to transparency\n",
    "inverted_img = inverted_img.convert(\"RGBA\")\n",
    "datas = inverted_img.getdata()\n",
    "\n",
    "new_data = []\n",
    "for item in datas:\n",
    "    # Change all white (also shades of whites)\n",
    "    # (255, 255, 255) to transparent\n",
    "    if item[0] > 200 and item[1] > 200 and item[2] > 200:\n",
    "        new_data.append((255, 255, 255, 0))\n",
    "    else:\n",
    "        new_data.append(item)\n",
    "\n",
    "# Update image data\n",
    "inverted_img.putdata(new_data)\n",
    "\n",
    "# Upscale the image\n",
    "upscaled_img = inverted_img.resize((img.width * 2, img.height * 2), Image.ANTIALIAS)\n",
    "\n",
    "# Save the output\n",
    "output_path = \"../homelab-status-page/static/images/blog/botparty.png\"\n",
    "upscaled_img.save(output_path)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c68d74cd-4132-463b-9db7-c1a4d233934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install playwright\n",
    "# ! pip install pyppeteer\n",
    "# ! pip install beautifulsoup4\n",
    "\n",
    "\n",
    "\"https://dynamicland.org/\" \n",
    "\"https://news.ycombinator.com/item?id=11803165\"\n",
    "\"https://worrydream.com/Shelf2015/\"\n",
    "\n",
    "#https://www.google.com/search?q=css+parsing+library&sourceid=chrome&ie=UTF-8\n",
    "# get 500 text from authors in 5 fields (design, econ, robotics, cognition, govtech(philosophy), math_of_causality)\n",
    "# get 500 papers from waymo/zoox/fbai/attention_paper/openai\n",
    "# pass each sentence through Comic-Graphics-Cognition Engine\n",
    "# give me {interconnections , visualizations-dynamic, content,metadata,}\n",
    "# eta sep 21\n",
    "\n",
    "#MVP = prompt -> lottie for JP \n",
    "dynamic_land_bookshelf = [\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9781477325766\",\n",
    "        \"title\": \"Before Writing\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9780521292429\",\n",
    "        \"title\": \"The Domestication of the Savage Mind\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://archive.org/details/in.gov.ignca.12550\",\n",
    "        \"title\": \"Writing\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9780674699069\",\n",
    "        \"title\": \"Preface to Plato\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://www.routledge.com/Orality-and-Literacy-30th-Anniversary-Edition/Ong/p/book/9780415538381\",\n",
    "        \"title\": \"Orality and Literacy\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9780521299558\",\n",
    "        \"title\": \"The Printing Press as an Agent of Change\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://en.wikipedia.org/wiki/The_Educated_Mind\",\n",
    "        \"title\": \"The Educated Mind\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://www.edwardtufte.com/book/envisioning-information/\",\n",
    "        \"title\": \"Envisioning Information\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://en.wikipedia.org/wiki/Understanding_Comics\",\n",
    "        \"title\": \"Understanding Comics\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://mitpress.mit.edu/9780262581462/cognition-in-the-wild/\",\n",
    "        \"title\": \"Cognition in the Wild\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9780143117469\",\n",
    "        \"title\": \"Shop Class as Soulcraft\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9780679740476\",\n",
    "        \"title\": \"The Hand\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://en.wikipedia.org/wiki/A_Pattern_Language\",\n",
    "        \"title\": \"A Pattern Language\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://en.wikipedia.org/wiki/The_Oregon_Experiment\",\n",
    "        \"title\": \"The Oregon Experiment\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://en.wikipedia.org/wiki/How_Buildings_Learn\",\n",
    "        \"title\": \"How Buildings Learn\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9780226113470\",\n",
    "        \"title\": \"Something Incredibly Wonderful Happens\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9780679764892\",\n",
    "        \"title\": \"Mr. Wilson's Cabinet of Wonder\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"http://www.newmediareader.com/\",\n",
    "        \"title\": \"The New Media Reader\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9780125232708\",\n",
    "        \"title\": \"From Memex to Hypertext\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9781783083442\",\n",
    "        \"title\": \"Memory Machines\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://worrydream.com/refs/Nelson_T_1974_-_Computer_Lib,_Dream_Machines.pdf\",\n",
    "        \"title\": \"Computer Lib / Dream Machines\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://en.wikipedia.org/wiki/Literary_Machines\",\n",
    "        \"title\": \"Literary Machines\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9781541675124\",\n",
    "        \"title\": \"Mindstorms\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://worrydream.com/refs/Allen-Conn_2003_-_Powerful_Ideas_in_the_Classroom.pdf\",\n",
    "        \"title\": \"Powerful Ideas in the Classroom\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://www.rheingold.com/texts/tft/\",\n",
    "        \"title\": \"Tools for Thought\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://en.wikipedia.org/wiki/Sketchpad\",\n",
    "        \"title\": \"Sketchpad\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://dougengelbart.org/content/view/201/\",\n",
    "        \"title\": \"The Augmentation Papers\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://dl.acm.org/doi/book/10.1145/61975\",\n",
    "        \"title\": \"A History of Personal Workstations\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://worrydream.com/refs/Krasner_1983_-_Smalltalk-80_Bits_of_History,_Words_of_Advice.pdf\",\n",
    "        \"title\": \"Smalltalk-80\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://archive.org/details/humaninterfacewh0000bolt\",\n",
    "        \"title\": \"The Human Interface\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://worrydream.com/refs/Bolt_1979_-_Spatial_Data_Management.pdf\",\n",
    "        \"title\": \"Spatial Data Management\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://worrydream.com/refs/Kim_1988_-_Viewpoint,_Toward_a_Computer_for_Visual_Thinkers.pdf\",\n",
    "        \"title\": \"Viewpoint\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://en.wikipedia.org/wiki/Seeing_Like_a_State\",\n",
    "        \"title\": \"Seeing Like a State\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://mitpress.mit.edu/9780262546799/simulation-and-its-discontents/\",\n",
    "        \"title\": \"Simulation and its Discontents\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9780143036531\",\n",
    "        \"title\": \"Amusing Ourselves to Death\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9780679745402\",\n",
    "        \"title\": \"Technopoly\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9781842300114\",\n",
    "        \"title\": \"Tools for Conviviality\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://web.stanford.edu/dept/SUL/sites/mac/primary/docs/satori/\",\n",
    "        \"title\": \"From Satori to Silicon Valley\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9781732265110\",\n",
    "        \"title\": \"The Dream Machine\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9780804738712\",\n",
    "        \"title\": \"Bootstrapping\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://worrydream.com/refs/Piumarta_2010_-_Points_of_View.pdf\",\n",
    "        \"title\": \"Points of View\"\n",
    "    },\n",
    "    {\n",
    "        \"href\": \"https://bookshop.org/book/9781849901154\",\n",
    "        \"title\": \"The Ascent of Man\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b6a0b-43ae-4dd8-a9d1-63e89b3ee8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "worry_Dream_bookshelf = []\n",
    "alan_kay_bookshelf = []\n",
    "# amazon bookshelf\n",
    "subjects = [\n",
    "    'robotics',\n",
    "    'design+cognition',\n",
    "    'future-economics',\n",
    "    'systems-thinking',\n",
    "    'math_of_causality'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1176c438-cfa4-4320-bd6c-176216c53ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "economics = [\n",
    "    \n",
    "    'https://pmarchive.com/', 'https://pmarca.substack.com/', 'https://a16z.com/author/marc-andreessen/'\n",
    "    \n",
    "    'https://people.ischool.berkeley.edu/~hal/writing.html', \n",
    "             'https://www.kalzumeus.com'\n",
    "             #'against the gods'\n",
    "            ]\n",
    "# youtube, blogs, pdocasts, books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da95b4f-642a-43aa-801a-5657409a4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sci-hub.ru/\n",
    "# https://github.com/topics/google-scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "951e1994-0822-4459-9e64-9c0c98cd48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.apify.com/best-google-scholar-apis-scrapers/\n",
    "# ehttps://github.com/topics/papers\n",
    "# https://github.com/topics/google-scholar\n",
    "# https://github.com/topics/scihub\n",
    "# https://github.com/topics/crossref\n",
    "# https://github.com/topics/scholar\n",
    "# you remember shocking people like tulac, katsnelson, everyone people you worked with\n",
    "# that was with an hour of work - what would happen if you spent 20,000 hours on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a99df5-dd85-4a17-9f94-29f20f1b22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m PyPaperBot  --scholar-pages=1 --scholar-results=20 --dwn-dir=/pdfs --proxy http://1.1.1.1::8080 https://8.8.8.8::8080\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "811fe14c-0f12-44b8-a922-10617a84827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPaperBot\n",
      "  Downloading PyPaperBot-1.2.2-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting astroid<=2.5,>=2.4.2 (from PyPaperBot)\n",
      "  Downloading astroid-2.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from PyPaperBot) (4.12.3)\n",
      "Collecting bibtexparser>=1.2.0 (from PyPaperBot)\n",
      "  Downloading bibtexparser-1.4.1.tar.gz (55 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2020.6.20 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from PyPaperBot) (2024.8.30)\n",
      "Collecting chardet>=3.0.4 (from PyPaperBot)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: colorama>=0.4.3 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from PyPaperBot) (0.4.6)\n",
      "Collecting crossref-commons>=0.0.7 (from PyPaperBot)\n",
      "  Downloading crossref_commons-0.0.7-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting future>=0.18.2 (from PyPaperBot)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting HTMLParser>=0.0.2 (from PyPaperBot)\n",
      "  Downloading HTMLParser-0.0.2.tar.gz (6.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting idna<3,>=2.10 (from PyPaperBot)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting isort>=5.4.2 (from PyPaperBot)\n",
      "  Downloading isort-5.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting lazy-object-proxy>=1.4.3 (from PyPaperBot)\n",
      "  Downloading lazy_object_proxy-1.10.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
      "Collecting mccabe>=0.6.1 (from PyPaperBot)\n",
      "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from PyPaperBot) (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from PyPaperBot) (2.2.2)\n",
      "Collecting pyChainedProxy>=1.1 (from PyPaperBot)\n",
      "  Downloading pyChainedProxy-1.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pylint>=2.6.0 (from PyPaperBot)\n",
      "  Downloading pylint-3.2.7-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from PyPaperBot) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from PyPaperBot) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from PyPaperBot) (2024.1)\n",
      "Collecting ratelimit>=2.2.1 (from PyPaperBot)\n",
      "  Downloading ratelimit-2.2.1.tar.gz (5.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.24.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from PyPaperBot) (2.32.3)\n",
      "Requirement already satisfied: six>=1.15.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from PyPaperBot) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>=2.0.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from PyPaperBot) (2.5)\n",
      "Collecting toml>=0.10.1 (from PyPaperBot)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from PyPaperBot) (1.26.20)\n",
      "Collecting wrapt>=1.12.1 (from PyPaperBot)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from pylint>=2.6.0->PyPaperBot) (4.2.2)\n",
      "INFO: pip is looking at multiple versions of pylint to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pylint>=2.6.0 (from PyPaperBot)\n",
      "  Downloading pylint-3.2.6-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-3.2.5-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-3.2.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-3.2.2-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-3.2.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "INFO: pip is still looking at multiple versions of pylint to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading pylint-3.1.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-3.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-3.0.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-3.0.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-3.0.2-py3-none-any.whl.metadata (12 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading pylint-3.0.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-3.0.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-2.17.7-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-2.17.6-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-2.17.5-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-2.17.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-2.17.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-2.17.2-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-2.17.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-2.17.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading pylint-2.16.4-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading pylint-2.16.3-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading pylint-2.16.2-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading pylint-2.16.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading pylint-2.16.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading pylint-2.15.10-py3-none-any.whl.metadata (9.9 kB)\n",
      "  Downloading pylint-2.15.9-py3-none-any.whl.metadata (9.9 kB)\n",
      "  Downloading pylint-2.15.8-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting dill>=0.2 (from pylint>=2.6.0->PyPaperBot)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pylint>=2.6.0 (from PyPaperBot)\n",
      "  Downloading pylint-2.15.7-py3-none-any.whl.metadata (9.8 kB)\n",
      "  Downloading pylint-2.15.6-py3-none-any.whl.metadata (9.8 kB)\n",
      "  Downloading pylint-2.15.5-py3-none-any.whl.metadata (9.8 kB)\n",
      "  Downloading pylint-2.15.4-py3-none-any.whl.metadata (9.8 kB)\n",
      "  Downloading pylint-2.15.3-py3-none-any.whl.metadata (9.8 kB)\n",
      "  Downloading pylint-2.15.2-py3-none-any.whl.metadata (9.8 kB)\n",
      "  Downloading pylint-2.15.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "  Downloading pylint-2.14.5-py3-none-any.whl.metadata (9.6 kB)\n",
      "  Downloading pylint-2.14.4-py3-none-any.whl.metadata (9.6 kB)\n",
      "  Downloading pylint-2.14.3-py3-none-any.whl.metadata (9.6 kB)\n",
      "  Downloading pylint-2.14.2-py3-none-any.whl.metadata (9.6 kB)\n",
      "  Downloading pylint-2.14.1-py3-none-any.whl.metadata (9.6 kB)\n",
      "  Downloading pylint-2.14.0-py3-none-any.whl.metadata (9.6 kB)\n",
      "  Downloading pylint-2.13.9-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading pylint-2.13.8-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading pylint-2.13.7-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading pylint-2.13.6-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading pylint-2.13.5-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading pylint-2.13.4-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading pylint-2.13.3-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading pylint-2.13.2-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading pylint-2.13.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading pylint-2.13.0-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading pylint-2.12.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading pylint-2.12.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading pylint-2.12.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading pylint-2.11.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading pylint-2.11.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading pylint-2.10.2-py3-none-any.whl.metadata (7.7 kB)\n",
      "  Downloading pylint-2.10.1-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from pylint>=2.6.0->PyPaperBot) (1.4.4)\n",
      "  Downloading pylint-2.10.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "  Downloading pylint-2.9.6-py3-none-any.whl.metadata (7.6 kB)\n",
      "  Downloading pylint-2.9.5-py3-none-any.whl.metadata (7.6 kB)\n",
      "  Downloading pylint-2.9.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "  Downloading pylint-2.9.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "  Downloading pylint-2.9.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "  Downloading pylint-2.9.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "  Downloading pylint-2.9.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "  Downloading pylint-2.8.3-py3-none-any.whl.metadata (7.4 kB)\n",
      "  Downloading pylint-2.8.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Downloading pylint-2.8.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Downloading pylint-2.8.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading pylint-2.7.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "  Downloading pylint-2.7.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "  Downloading pylint-2.7.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "  Downloading pylint-2.7.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting mccabe>=0.6.1 (from PyPaperBot)\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from requests>=2.24.0->PyPaperBot) (3.3.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from pandas->PyPaperBot) (2024.1)\n",
      "Downloading PyPaperBot-1.2.2-py3-none-any.whl (14 kB)\n",
      "Downloading astroid-2.5-py3-none-any.whl (220 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading crossref_commons-0.0.7-py3-none-any.whl (14 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Downloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
      "Downloading lazy_object_proxy-1.10.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68 kB)\n",
      "Downloading pyChainedProxy-1.3-py3-none-any.whl (16 kB)\n",
      "Downloading pylint-2.7.1-py3-none-any.whl (343 kB)\n",
      "Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: bibtexparser, HTMLParser, ratelimit, wrapt\n",
      "  Building wheel for bibtexparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bibtexparser: filename=bibtexparser-1.4.1-py3-none-any.whl size=43253 sha256=3bf6001ebeb1235077243d4e2236318d7b77950dac3f79f580de3eeffc239953\n",
      "  Stored in directory: /home/adnan/.cache/pip/wheels/08/c6/c3/56e639fab68d1fdbf13ea147636d9795ccdbd3c1d3178d1332\n",
      "  Building wheel for HTMLParser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for HTMLParser: filename=HTMLParser-0.0.2-py3-none-any.whl size=5981 sha256=cc0398a455ee9173dbaa08483fde9b2c301f9f314a1510931ea06920cf145083\n",
      "  Stored in directory: /home/adnan/.cache/pip/wheels/b9/35/b7/11948e6116d1398b6af31bf5a89ba052a34df97cd79ace9144\n",
      "  Building wheel for ratelimit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ratelimit: filename=ratelimit-2.2.1-py3-none-any.whl size=5893 sha256=ca626bea68ca0a2aa9c248a021b48172df9c716c61e84264ffc056340e0ae1b2\n",
      "  Stored in directory: /home/adnan/.cache/pip/wheels/27/5f/ba/e972a56dcbf5de9f2b7d2b2a710113970bd173c4dcd3d2c902\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp310-cp310-linux_x86_64.whl size=36664 sha256=97441f5967f3f84db0aa319dadeb439fe831de3d79c9fb5170512bba2a4867eb\n",
      "  Stored in directory: /home/adnan/.cache/pip/wheels/8e/61/d3/d9e7053100177668fa43216a8082868c55015f8706abd974f2\n",
      "Successfully built bibtexparser HTMLParser ratelimit wrapt\n",
      "Installing collected packages: wrapt, ratelimit, pyChainedProxy, mccabe, HTMLParser, toml, lazy-object-proxy, isort, idna, future, chardet, bibtexparser, astroid, pylint, crossref-commons, PyPaperBot\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.8\n",
      "    Uninstalling idna-3.8:\n",
      "      Successfully uninstalled idna-3.8\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "retell 0.5.1 requires idna>=3.4, but you have idna 2.10 which is incompatible.\n",
      "yt-dlp 2024.8.6 requires websockets>=12.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed HTMLParser-0.0.2 PyPaperBot-1.2.2 astroid-2.5 bibtexparser-1.4.1 chardet-5.2.0 crossref-commons-0.0.7 future-1.0.0 idna-2.10 isort-5.13.2 lazy-object-proxy-1.10.0 mccabe-0.6.1 pyChainedProxy-1.3 pylint-2.7.1 ratelimit-2.2.1 toml-0.10.2 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "! pip install PyPaperBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2709ca1-612d-4d83-8f79-d8286d02393a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91dbde00-ba3c-471e-946e-ca6745a9015a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9de704-9fdf-476c-b3d5-6f2ac8eba1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc46f41d-472c-4c70-94e7-4fa14120df17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a2219-96d6-4ad6-8dcf-742d811610e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3d742-5083-462d-a48a-149a93c75ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5691a81-4e18-4fb6-ba88-785edb1230ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task pending name='Task-6' coro=<main() running at /tmp/ipykernel_680067/3782346540.py:46>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, elajjaz\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "961fb71b-6ea3-4000-ac43-14243c4cc95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# worry_dream = 'https://worrydream.com/' \n",
    "# paul_graham = 'https://paulgraham.com/'\n",
    "links = [\n",
    "    \"https://norvig.com/\",\n",
    "    \n",
    "    # paul_graham, \n",
    "    # worry_dream\n",
    "    # 1. alan kay - (science)\n",
    "# 2. bret victor - (design)\n",
    "# 3. enjalot - py / js\n",
    "# 4. jeremey howard - py (gpu poor)\n",
    "# 5. carmack - C++ / graphics (mikola)\n",
    "# 6. tailcale people - golang (networking)\n",
    "# 7. Norvig - python + problem solving (paip)\n",
    "# 8. Paul graham - (documentation + lisp + sicp)\n",
    "# 9. robots --— ????? (roomba, botparty, swarmbotics, unitree, huggingface?)\n",
    "# 10. zoox - robots\n",
    "]\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "def parse_links_with_bs4(links):\n",
    "    result = {}\n",
    "\n",
    "    for link in links:\n",
    "        try:\n",
    "            # Get the base hostname\n",
    "            parsed_link = urlparse(link)\n",
    "            base_url = f\"{parsed_link.scheme}://{parsed_link.netloc}\"\n",
    "            result[link] = {\"links\": [], \"texts\": []}\n",
    "\n",
    "            # Fetch the page content\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Get all links with the same hostname\n",
    "            all_links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "            same_host_links = [urljoin(base_url, url) for url in all_links if urlparse(urljoin(base_url, url)).netloc == parsed_link.netloc]\n",
    "\n",
    "            # Avoid duplicates\n",
    "            unique_links = list(set(same_host_links))\n",
    "            result[link][\"links\"] = unique_links\n",
    "\n",
    "            # Fetch text for the base page\n",
    "            result[link][\"texts\"].append({\n",
    "                \"url\": link,\n",
    "                \"text\": soup.get_text(strip=True)\n",
    "            })\n",
    "\n",
    "            # Fetch text from each link within the same hostname\n",
    "            for inner_link in unique_links:\n",
    "                try:\n",
    "                    inner_response = requests.get(inner_link)\n",
    "                    inner_soup = BeautifulSoup(inner_response.content, 'html.parser')\n",
    "                    text_content = inner_soup.get_text(strip=True)\n",
    "                    result[link][\"texts\"].append({\n",
    "                        \"url\": inner_link,\n",
    "                        \"text\": text_content\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to fetch {inner_link}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {link}: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage in Jupyter\n",
    "\n",
    "\n",
    "# Run the function and get the results\n",
    "results = parse_links_with_bs4(links)\n",
    "\n",
    "# Display results in the notebook\n",
    "# for original_link, data in results.items():\n",
    "#     print(f\"Original URL: {original_link}\")\n",
    "#     print(\"Links found:\")\n",
    "#     for l in data[\"links\"]:\n",
    "#         print(l)\n",
    "#     print(\"Text from each link:\")\n",
    "#     for t in data[\"texts\"]:\n",
    "#         print(f\"URL: {t['url']}\\nText: {t['text'][:100]}...\") \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Print first 100 characters for brevity\n",
    "\n",
    "\n",
    "\n",
    "#zenkaisen - note taking method - mind maps - arxiv explororer /??\n",
    "print(len(json.dumps(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cfb937d-e1c5-40fe-a179-282dbc275e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429\n"
     ]
    }
   ],
   "source": [
    "#results = {}\n",
    "#! cp living_research_papers.json living_research_papers-2.json\n",
    "#! mkdir pdfs\n",
    "#! ls pdfs\n",
    "\n",
    "def find_and_download_pdfs(folder_path=\"pdfs\"):\n",
    "    link = 'https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:KlAtU1dfN6UC'\n",
    "    response = requests.get(link)\n",
    "    print(response.status_code)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all <a> tags that have a span with text \"[PDF]\"\n",
    "        print(response.text)\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            print(a_tag)\n",
    "            span_tag = a_tag.find('span')\n",
    "            print('please work fucker')\n",
    "            if span_tag and \"[PDF]\" in span_tag.text:\n",
    "                pdf_url = a_tag['href']\n",
    "                # Handle relative links if necessary\n",
    "                pdf_url = pdf_url if pdf_url.startswith('http') else requests.compat.urljoin(link, pdf_url)\n",
    "                download_pdf(pdf_url, folder_path)\n",
    "\n",
    "find_and_download_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2532976-e032-499c-a7bf-4b8870a8edb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 50\u001b[0m\n\u001b[1;32m     44\u001b[0m links \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:KlAtU1dfN6UC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Add more links here\u001b[39;00m\n\u001b[1;32m     47\u001b[0m ]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Run the scraping with Playwright\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mscrape_with_playwright\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 23\u001b[0m, in \u001b[0;36mscrape_with_playwright\u001b[0;34m(links)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_with_playwright\u001b[39m(links):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m sync_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m     24\u001b[0m         browser \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mfirefox\u001b[38;5;241m.\u001b[39mlaunch(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m         context \u001b[38;5;241m=\u001b[39m browser\u001b[38;5;241m.\u001b[39mnew_context()\n",
      "File \u001b[0;32m~/micromamba/envs/sam/lib/python3.10/site-packages/playwright/sync_api/_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m             )\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# Function to download PDF from a URL\n",
    "def download_pdf(pdf_url, folder_path=\"pdfs\"):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    response = requests.get(pdf_url)\n",
    "    if response.status_code == 200:\n",
    "        pdf_name = pdf_url.split(\"/\")[-1]\n",
    "        pdf_path = os.path.join(folder_path, pdf_name)\n",
    "        \n",
    "        with open(pdf_path, 'wb') as pdf_file:\n",
    "            pdf_file.write(response.content)\n",
    "        print(f\"Downloaded: {pdf_name}\")\n",
    "    else:\n",
    "        print(f\"No PDF found at: {pdf_url}\")\n",
    "\n",
    "# Function to scrape PDF links with Playwright\n",
    "def scrape_with_playwright(links):\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.firefox.launch(headless=True)\n",
    "        context = browser.new_context()\n",
    "\n",
    "        for link in links:\n",
    "            page = context.new_page()\n",
    "            page.goto(link)\n",
    "\n",
    "            # Find all links with [PDF] span\n",
    "            a_tags = page.query_selector_all('a:has(span:text-is(\"[PDF]\"))')\n",
    "\n",
    "            for a_tag in a_tags:\n",
    "                pdf_url = a_tag.get_attribute('href')\n",
    "                if pdf_url:\n",
    "                    pdf_url = pdf_url if pdf_url.startswith('http') else page.urljoin(pdf_url)\n",
    "                    download_pdf(pdf_url)\n",
    "\n",
    "            page.close()\n",
    "        browser.close()\n",
    "\n",
    "\n",
    "# Example list of links to scrape\n",
    "links = [\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:KlAtU1dfN6UC\",\n",
    "    # Add more links here\n",
    "]\n",
    "async def main():\n",
    "    await scrape_with_playwright(links)\n",
    "\n",
    "# Entry point for the async function\n",
    "asyncio.run(main())\n",
    "# Run the scraping with Playwright\n",
    "#scrape_with_playwright(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02f4586c-a2b4-4920-a6a1-f52d758674b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Function to download PDF from a URL\n",
    "def download_pdf(pdf_url, folder_path=\"pdfs\"):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    response = requests.get(pdf_url)\n",
    "    if response.status_code == 200 and \"application/pdf\" in response.headers.get('content-type', ''):\n",
    "        pdf_name = pdf_url.split(\"/\")[-1]\n",
    "        pdf_path = os.path.join(folder_path, pdf_name)\n",
    "        \n",
    "        with open(pdf_path, 'wb') as pdf_file:\n",
    "            pdf_file.write(response.content)\n",
    "        print(f\"Downloaded: {pdf_name}\")\n",
    "    else:\n",
    "        print(f\"No PDF found at: {pdf_url}\")\n",
    "\n",
    "\n",
    "# Function to find PDF links using the span \"[PDF]\" decoration\n",
    "def find_and_download_pdfs(links, folder_path=\"pdfs\"):\n",
    "    for link in links:\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all <a> tags that have a span with text \"[PDF]\"\n",
    "            for a_tag in soup.find_all('a', href=True):\n",
    "                span_tag = a_tag.find('span')\n",
    "                print('please work fucker')\n",
    "                if span_tag and \"[PDF]\" in span_tag.text:\n",
    "                    pdf_url = a_tag['href']\n",
    "                    # Handle relative links if necessary\n",
    "                    pdf_url = pdf_url if pdf_url.startswith('http') else requests.compat.urljoin(link, pdf_url)\n",
    "                    download_pdf(pdf_url, folder_path)\n",
    "\n",
    "\n",
    "# Example list of links from Google Scholar\n",
    "links = [\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:KlAtU1dfN6UC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:u5HHmVD_uO8C\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:3fE2CSJIrl8C\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:u-x6o8ySG0sC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:2osOgNQ5qMEC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:0EnyYjriUFMC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:9ZlFYXVOiuMC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:d1gkVwhDpl0C\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:IWHjjKOFINEC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:IjCSPb-OGe4C\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:UeHWp8X0CEIC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:Zph67rFs4hoC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:7PzlFSSx8tAC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:zYLM7Y9cAGgC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:UebtZRa9Y70C\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:9yKSN-GCB0IC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:WF5omc3nYNoC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:eQOLeE2rZwMC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:qjMakFHDy7sC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&citation_for_view=ZKtYLHwAAAAJ:qUcmZB5y_30C\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&cstart=20&pagesize=80&citation_for_view=ZKtYLHwAAAAJ:hqOjcs7Dif8C\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&cstart=20&pagesize=80&citation_for_view=ZKtYLHwAAAAJ:ULOm3_A8WrAC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&cstart=20&pagesize=80&citation_for_view=ZKtYLHwAAAAJ:roLk4NBRz8UC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&cstart=20&pagesize=80&citation_for_view=ZKtYLHwAAAAJ:YOwf2qJgpHMC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&cstart=20&pagesize=80&citation_for_view=ZKtYLHwAAAAJ:YsMSGLbcyi4C\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&cstart=20&pagesize=80&citation_for_view=ZKtYLHwAAAAJ:ufrVoPGSRksC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&cstart=20&pagesize=80&citation_for_view=ZKtYLHwAAAAJ:LkGwnXOMwfcC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&cstart=20&pagesize=80&citation_for_view=ZKtYLHwAAAAJ:dhFuZR0502QC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&cstart=20&pagesize=80&citation_for_view=ZKtYLHwAAAAJ:W7OEmFMy1HYC\",\n",
    "    \"https://scholar.google.de/citations?view_op=view_citation&hl=de&user=ZKtYLHwAAAAJ&cstart=20&pagesize=80&citation_for_view=ZKtYLHwAAAAJ:_FxGoFyzp5QC\"\n",
    "]\n",
    "\n",
    "# Call the function to scrape for PDFs and download them\n",
    "find_and_download_pdfs(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d92510c8-7a32-4505-909b-72a066288889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8968263\n",
      "10344721\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"living_research_papers-2.json\"\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "except FileNotFoundError:\n",
    "    data = {}  # If the file doesn't exist, initialize with an empty list\n",
    "\n",
    "print(len(json.dumps(data)))\n",
    "# Append new results to the existing data\n",
    "#data.append(results)\n",
    "data.update(results)\n",
    "print(len(json.dumps(data)))\n",
    "\n",
    "# Save the updated JSON data back to the file system\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008f4f1-e748-4381-b77e-69dae3e8dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shit = 'https://scholar.google.de/citations?user=ZKtYLHwAAAAJ&hl=de'\n",
    "#Array.from(document.body.querySelectorAll('.gsc_a_t a')).map(_=> _.href).slice(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb62ba5c-bdd1-4ce5-9cf9-4ae3560d624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"https://scholar.google.de/citations?user=ZKtYLHwAAAAJ&hl=de\": {\n",
      "        \"from\": \"zoox\",\n",
      "        \"links\": [\n",
      "            \"#\",\n",
      "            \"//www.google.com/policies/terms/\",\n",
      "            \"//support.google.com/websearch/answer/86640\"\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_google_scholar(url, source_name):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all links on the page\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    \n",
    "    # Create the JSON structure\n",
    "    result = {\n",
    "        url: {\n",
    "            \"from\": source_name,\n",
    "            \"links\": links\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Return the JSON object\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "url = \"https://scholar.google.de/citations?user=ZKtYLHwAAAAJ&hl=de\"\n",
    "source_name = \"zoox\"\n",
    "scraped_data = scrape_google_scholar(url, source_name)\n",
    "\n",
    "# Save the scraped data to a JSON file\n",
    "file_path = \"scholar_links.json\"\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(scraped_data, json_file, indent=4)\n",
    "\n",
    "# Print the result\n",
    "print(json.dumps(scraped_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b689e0b-c17d-440e-8b9b-26bf6966d638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "def parse_links_with_bs4(links, max_depth=20):\n",
    "    result = {}\n",
    "\n",
    "    def fetch_page_data(url, base_url, current_depth):\n",
    "        try:\n",
    "            # Fetch the page content\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extract all links from the page and filter those with the same hostname\n",
    "            all_links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "            same_host_links = [urljoin(base_url, l) for l in all_links if urlparse(urljoin(base_url, l)).netloc == urlparse(base_url).netloc]\n",
    "\n",
    "            # Avoid duplicates\n",
    "            unique_links = list(set(same_host_links))\n",
    "\n",
    "            # Get the inner text from the page's body\n",
    "            page_content = soup.body.get_text(strip=True) if soup.body else \"\"\n",
    "\n",
    "            # Store the content and links with depth in the result\n",
    "            result[url] = {\n",
    "                \"content\": page_content,\n",
    "                \"links\": unique_links,\n",
    "                \"depth\": current_depth\n",
    "            }\n",
    "\n",
    "            # Recurse through the links if depth is within the limit\n",
    "            if current_depth < max_depth:\n",
    "                for inner_link in unique_links:\n",
    "                    if inner_link not in result:  # Avoid revisiting the same link\n",
    "                        fetch_page_data(inner_link, base_url, current_depth + 1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "\n",
    "    # Process each link from the input\n",
    "    for link in links:\n",
    "        base_url = f\"{urlparse(link).scheme}://{urlparse(link).netloc}\"\n",
    "        fetch_page_data(link, base_url, 0)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage in Jupyter\n",
    "links = ['https://worrydream.com', paul_graham]\n",
    "\n",
    "# Run the function and get the results\n",
    "results = parse_links_with_bs4(links)\n",
    "\n",
    "# Display results in the notebook\n",
    "import json\n",
    "print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d4b11-cdeb-4524-bdb1-da642a0733a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n",
    "##save this to server "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc982e71-d1a1-42fe-a4e8-57fe7bac3995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5e257-5580-4370-a6ad-29853474b1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633cbb4-d034-426b-966f-584a63723ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356194c9-b000-4ad0-840e-565a026c9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# deliverable = text field that sends each line to function -> generates 10 gifs / lotties\n",
    "\n",
    "#parse entire website\n",
    "#alan kay how \n",
    "#jeffrey heer \n",
    "\n",
    "#query that\n",
    "\n",
    "\n",
    "# without good data - ai + humans = kinda useles\n",
    "# doesnt matter how many gpus or brain cells you have \n",
    "# alan kay - da vicini couldnt invent a flying machine but he imagined it many times\n",
    "\n",
    "# only when newton discovered physics - could the wright brothers do anything \n",
    "\n",
    "\n",
    "# newton stood on gallileos' shouldrs\n",
    "# gauss stood on netwons's shoulders\n",
    "# von neumann stood on gaus's shoulders\n",
    "\n",
    "\n",
    "# changing the wrold isnt hard\n",
    "# but chnanign in the maximamlly beneficiall to all way = more challenging\n",
    "# inventing robots = cool - but if designed without intention - it could deltee more jbos than it creates short-term\n",
    "\n",
    "\n",
    "# but properly designed - robots can make more jobs tomorrow \n",
    "# to change the world in a good way - must see it accurately \n",
    "# seeing = limited bandwidth - cant know everything - only can hold 7-20 items in our memory \n",
    "\n",
    "# knowing what to learn and in what order = difficult but important probelm with super-linear  returns \n",
    "\n",
    "# ride in on titdal wave = how all billion dollar companies are made - \n",
    "# its not just luck for sure, but its not just merit either. \n",
    "\n",
    "#! wget https://internetat50.com/references/Kay_How.pdf\n",
    "\n",
    "# get all robotics ocmpanies in industry\n",
    "\n",
    "# find industries that can benefit from robotics\n",
    "# obs didnt rocketship bc they couldnt predict gpu+llm->no one could\n",
    "# now everyone can do python - gpt fills in gaps\n",
    "# and any non-python firnedly tooling = swamp \n",
    "# https://www.ycombinator.com/companies\n",
    "# https://www.ycombinator.com/companies/hey-revia\n",
    "# https://www.ycombinator.com/companies/videogen\n",
    "# https://www.ycombinator.com/companies/pinnacle\n",
    "# https://www.ycombinator.com/companies/kisho\n",
    "# https://www.ycombinator.com/companies/ficra\n",
    "# https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "244f4206-2fa5-4da5-a598-bf7e7d957cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nbdime\n",
      "  Downloading nbdime-4.0.2-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting colorama (from nbdime)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gitpython!=2.1.4,!=2.1.5,!=2.1.6 (from nbdime)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jinja2>=2.9 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbdime) (3.1.4)\n",
      "Requirement already satisfied: jupyter-server in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbdime) (2.14.2)\n",
      "Collecting jupyter-server-mathjax>=0.2.2 (from nbdime)\n",
      "  Downloading jupyter_server_mathjax-0.2.6-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: nbformat in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbdime) (5.10.4)\n",
      "Requirement already satisfied: pygments in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbdime) (2.18.0)\n",
      "Requirement already satisfied: requests in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbdime) (2.32.3)\n",
      "Requirement already satisfied: tornado in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbdime) (6.4.1)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=2.1.4,!=2.1.5,!=2.1.6->nbdime)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jinja2>=2.9->nbdime) (2.1.5)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (4.4.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (8.6.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (7.16.4)\n",
      "Requirement already satisfied: overrides>=5.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (7.7.0)\n",
      "Requirement already satisfied: packaging>=22.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (24.1)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (0.20.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (26.2.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (0.18.1)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (5.14.3)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server->nbdime) (1.8.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbformat->nbdime) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbformat->nbdime) (4.23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from requests->nbdime) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from requests->nbdime) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from requests->nbdime) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from requests->nbdime) (2024.8.30)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server->nbdime) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server->nbdime) (1.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server->nbdime) (4.12.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server->nbdime) (21.2.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=2.1.4,!=2.1.5,!=2.1.6->nbdime)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->nbdime) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->nbdime) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->nbdime) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->nbdime) (0.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-client>=7.4.4->jupyter-server->nbdime) (2.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server->nbdime) (4.2.2)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server->nbdime) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server->nbdime) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server->nbdime) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server->nbdime) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime) (0.10.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime) (1.3.0)\n",
      "Requirement already satisfied: ptyprocess in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from terminado>=0.8.3->jupyter-server->nbdime) (0.7.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server->nbdime) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server->nbdime) (0.5.1)\n",
      "Requirement already satisfied: fqdn in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime) (3.0.0)\n",
      "Requirement already satisfied: uri-template in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime) (24.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server->nbdime) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server->nbdime) (2.5)\n",
      "Requirement already satisfied: pycparser in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server->nbdime) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime) (2.9.0.20240821)\n",
      "Downloading nbdime-4.0.2-py3-none-any.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading jupyter_server_mathjax-0.2.6-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, colorama, gitdb, gitpython, jupyter-server-mathjax, nbdime\n",
      "Successfully installed colorama-0.4.6 gitdb-4.0.11 gitpython-3.1.43 jupyter-server-mathjax-0.2.6 nbdime-4.0.2 smmap-5.0.1\n",
      "No .git directory in ., skipping git attributes\n",
      "No .git directory in ., skipping git attributes\n"
     ]
    }
   ],
   "source": [
    "! pip install nbdime\n",
    "! nbdime config-git --enable\n",
    "! pip install jupytext\n",
    "! pip install ipywidgets\n",
    "# pip install jupyter_contrib_nbextensions\n",
    "# jupyter contrib nbextension install --user\n",
    "# https://github.com/jupyter/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08b8fc82-725b-43c0-9a7f-6d2440b3b5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter-ai-magics\n",
      "  Downloading jupyter_ai_magics-2.23.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting click~=8.0 (from jupyter-ai-magics)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=5.2.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-ai-magics) (8.4.0)\n",
      "Requirement already satisfied: ipython in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-ai-magics) (8.27.0)\n",
      "Collecting jsonpath-ng<2,>=1.5.3 (from jupyter-ai-magics)\n",
      "  Downloading jsonpath_ng-1.6.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting langchain-community<0.3.0,>=0.1.0 (from jupyter-ai-magics)\n",
      "  Downloading langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain<0.3.0,>=0.1.0 (from jupyter-ai-magics)\n",
      "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-ai-magics) (4.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from importlib-metadata>=5.2.0->jupyter-ai-magics) (3.20.1)\n",
      "Collecting ply (from jsonpath-ng<2,>=1.5.3->jupyter-ai-magics)\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (6.0.2)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.38 (from langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading langchain_core-0.2.40-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading langsmith-0.1.121-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (2.9.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from langchain-community<0.3.0,>=0.1.0->jupyter-ai-magics) (0.6.7)\n",
      "Requirement already satisfied: decorator in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython->jupyter-ai-magics) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython->jupyter-ai-magics) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython->jupyter-ai-magics) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython->jupyter-ai-magics) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython->jupyter-ai-magics) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython->jupyter-ai-magics) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython->jupyter-ai-magics) (5.14.3)\n",
      "Requirement already satisfied: exceptiongroup in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython->jupyter-ai-magics) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython->jupyter-ai-magics) (4.9.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading yarl-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.1.0->jupyter-ai-magics) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.1.0->jupyter-ai-magics) (0.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jedi>=0.16->ipython->jupyter-ai-magics) (0.8.4)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.38->langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (24.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (0.27.2)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain<0.3.0,>=0.1.0->jupyter-ai-magics)\n",
      "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from pexpect>4.3->ipython->jupyter-ai-magics) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->jupyter-ai-magics) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (2.23.2)\n",
      "Requirement already satisfied: tzdata in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from requests<3,>=2->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (3.0.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from stack-data->ipython->jupyter-ai-magics) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from stack-data->ipython->jupyter-ai-magics) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from stack-data->ipython->jupyter-ai-magics) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython->jupyter-ai-magics) (1.16.0)\n",
      "Requirement already satisfied: anyio in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain<0.3.0,>=0.1.0->jupyter-ai-magics) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.1.0->jupyter-ai-magics) (1.0.0)\n",
      "Downloading jupyter_ai_magics-2.23.0-py3-none-any.whl (36 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading jsonpath_ng-1.6.1-py3-none-any.whl (29 kB)\n",
      "Downloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.2.17-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading langchain_core-0.2.40-py3-none-any.whl (396 kB)\n",
      "Downloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.121-py3-none-any.whl (289 kB)\n",
      "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Downloading yarl-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
      "Installing collected packages: ply, tenacity, SQLAlchemy, orjson, numpy, multidict, jsonpath-ng, jsonpatch, frozenlist, click, async-timeout, aiohappyeyeballs, yarl, aiosignal, langsmith, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain-community, jupyter-ai-magics\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.0\n",
      "    Uninstalling numpy-2.1.0:\n",
      "      Successfully uninstalled numpy-2.1.0\n",
      "Successfully installed SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 async-timeout-4.0.3 click-8.1.7 frozenlist-1.4.1 jsonpatch-1.33 jsonpath-ng-1.6.1 jupyter-ai-magics-2.23.0 langchain-0.2.16 langchain-community-0.2.17 langchain-core-0.2.40 langchain-text-splitters-0.2.4 langsmith-0.1.121 multidict-6.1.0 numpy-1.26.4 orjson-3.10.7 ply-3.11 tenacity-8.5.0 yarl-1.11.1\n"
     ]
    }
   ],
   "source": [
    "! pip install jupyter-ai-magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "799d77a8-3ac6-49e0-b0fc-2c1ce0bbd92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in ZMQInteractiveShell\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "kernel_name = get_ipython().__class__.__name__\n",
    "if 'lab' in kernel_name.lower():\n",
    "    print(\"Running in JupyterLab\")\n",
    "else:\n",
    "    print(f\"Running in {kernel_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dcbcc92-acbd-409e-ae62-333e253512d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in JupyterLab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if \"JUPYTERLAB_SETTINGS_DIR\" in os.environ:\n",
    "    print(\"Running in JupyterLab\")\n",
    "else:\n",
    "    print(\"Not running in JupyterLab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3744f2e0-9806-4c12-a6a7-0cc689d4da88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyterlab in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (4.2.5)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (0.27.2)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (6.29.5)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (3.1.4)\n",
      "Requirement already satisfied: jupyter-core in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (5.7.2)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (2.2.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (2.14.2)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (0.2.4)\n",
      "Requirement already satisfied: packaging in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (24.1)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (73.0.1)\n",
      "Requirement already satisfied: tomli>=1.2.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (2.0.1)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (6.4.1)\n",
      "Requirement already satisfied: traitlets in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab) (5.14.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from async-lru>=1.0.0->jupyterlab) (4.12.2)\n",
      "Requirement already satisfied: anyio in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab) (4.4.0)\n",
      "Requirement already satisfied: certifi in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab) (3.8)\n",
      "Requirement already satisfied: sniffio in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab) (0.14.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab) (1.8.5)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab) (8.27.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab) (8.6.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab) (1.6.0)\n",
      "Requirement already satisfied: psutil in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab) (6.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab) (26.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jinja2>=3.0.3->jupyterlab) (2.1.5)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-core->jupyterlab) (4.2.2)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.16.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.20.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (2.14.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (0.9.25)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (4.23.0)\n",
      "Requirement already satisfied: requests>=2.31 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (2.32.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from anyio->httpx>=0.25.0->jupyterlab) (1.2.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab) (21.2.0)\n",
      "Requirement already satisfied: decorator in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (4.9.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (0.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=6.5.0->jupyterlab) (2.9.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.10.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.3.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab) (1.26.20)\n",
      "Requirement already satisfied: ptyprocess in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (0.8.4)\n",
      "Requirement already satisfied: fqdn in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (3.0.0)\n",
      "Requirement already satisfied: uri-template in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (24.8.0)\n",
      "Requirement already satisfied: wcwidth in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (0.2.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (0.2.3)\n",
      "Requirement already satisfied: pycparser in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/adnan/micromamba/envs/sam/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.9.0.20240821)\n"
     ]
    }
   ],
   "source": [
    "! pip install jupyterlab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc74a626-3dac-4b69-8b33-5462cdb97670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa925efd-6727-4a4d-83b3-51086c006fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# # Set your OpenAI API key\n",
    "# openai.api_key = 'your-api-key'\n",
    "\n",
    "# def query_chatgpt(prompt):\n",
    "#     response = openai.Completion.create(\n",
    "#         model=\"text-davinci-003\", \n",
    "#         prompt=prompt,\n",
    "#         max_tokens=150\n",
    "#     )\n",
    "#     return response.choices[0].text.strip()\n",
    "\n",
    "# # Example usage\n",
    "# result = query_chatgpt(\"Tell me about quantum computing.\")\n",
    "# print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
